{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIKT_Tf_version_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQm/HFlxsHTS+Y91x9SLWF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankalpachowdhury/Candidate_Skill_Assessment_using_Graph_Neural_Network/blob/main/GIKT_Tf_version_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvn4T5fdq-K_"
      },
      "source": [
        "# Install Framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibqsb6mmzdIZ",
        "outputId": "611697b4-3ca2-4440-c6d7-2dec0e24c7f0"
      },
      "source": [
        "# Run only if needed\n",
        "!pip install tensorflow==2 #1.13.0rc1\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2 in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (2.0.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.34.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (2.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (3.12.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2) (1.0.8)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (57.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.31.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.4.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (4.5.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (1.3.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2) (3.1.1)\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsZWoUyB35L4",
        "outputId": "bcdc5766-e827-4492-eb6b-0925c96d6fb3"
      },
      "source": [
        "# run to check version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7mgqs0M2HVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b24ff06-a3a5-454a-c264-b7392ea1c8ac"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtsag8iXlxoA",
        "outputId": "a68fcd19-c91e-418e-808a-947f28395ad7"
      },
      "source": [
        "# gikt tf v2 new commit\n",
        "#https://github.com/ajenningsfrankston/graph_kt.git\n",
        "!git clone \"https://github.com/ajenningsfrankston/graph_kt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'graph_kt'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 28 (delta 7), reused 27 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8RM6kIXjlBc",
        "outputId": "be2e620d-14e0-49dc-dadf-463e7c009591"
      },
      "source": [
        "# import all codes\n",
        "!git clone \"https://github.com/Rimoku/GIKT.git\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GIKT'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 19 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC7TzZFRrZEY"
      },
      "source": [
        "\n",
        "# import codes from own github "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9l1hmiVp0xu"
      },
      "source": [
        "#DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FhweX93p27O",
        "outputId": "c2b17ea1-d096-4495-d563-141b0bc25134"
      },
      "source": [
        "#get the data from the gdrive\n",
        "\n",
        "# 1 mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDLIwwTsXSQ"
      },
      "source": [
        "# 2 copy the folders to local\n",
        "!cp -r /content/gdrive/MyDrive/FinalYear_Project_Dataset/data /content/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXnz4LCHqUlu"
      },
      "source": [
        "# create logs and checkpoint folders\n",
        "!mkdir logs\n",
        "!mkdir checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttNQQN3qdEX"
      },
      "source": [
        "#Convert to Tf v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09CNfHmmpvJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047c7e66-2882-4483-82c7-b2d4695b2b9b"
      },
      "source": [
        "!tf_upgrade_v2 -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: tf_upgrade_v2 [-h] [--infile INPUT_FILE] [--outfile OUTPUT_FILE]\n",
            "                     [--intree INPUT_TREE] [--outtree OUTPUT_TREE]\n",
            "                     [--copyotherfiles COPY_OTHER_FILES] [--inplace]\n",
            "                     [--import_rename] [--reportfile REPORT_FILENAME]\n",
            "                     [--mode {DEFAULT,SAFETY}] [--print_all]\n",
            "\n",
            "Convert a TensorFlow Python file from 1.x to 2.0\n",
            "\n",
            "Simple usage:\n",
            "  tf_upgrade_v2.py --infile foo.py --outfile bar.py\n",
            "  tf_upgrade_v2.py --infile foo.ipynb --outfile bar.ipynb\n",
            "  tf_upgrade_v2.py --intree ~/code/old --outtree ~/code/new\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --infile INPUT_FILE   If converting a single file, the name of the file to\n",
            "                        convert\n",
            "  --outfile OUTPUT_FILE\n",
            "                        If converting a single file, the output filename.\n",
            "  --intree INPUT_TREE   If converting a whole tree of files, the directory to\n",
            "                        read from (relative or absolute).\n",
            "  --outtree OUTPUT_TREE\n",
            "                        If converting a whole tree of files, the output\n",
            "                        directory (relative or absolute).\n",
            "  --copyotherfiles COPY_OTHER_FILES\n",
            "                        If converting a whole tree of files, whether to copy\n",
            "                        the other files.\n",
            "  --inplace             If converting a set of files, whether to allow the\n",
            "                        conversion to be performed on the input files.\n",
            "  --import_rename       Whether to rename import to compact.v2 explicitly.\n",
            "  --reportfile REPORT_FILENAME\n",
            "                        The name of the file where the report log is\n",
            "                        stored.(default: report.txt)\n",
            "  --mode {DEFAULT,SAFETY}\n",
            "                        Upgrade script mode. Supported modes: DEFAULT: Perform\n",
            "                        only straightforward conversions to upgrade to 2.0. In\n",
            "                        more difficult cases, switch to use compat.v1. SAFETY:\n",
            "                        Keep 1.* code intact and import compat.v1 module.\n",
            "  --print_all           Print full log to stdout instead of just printing\n",
            "                        errors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHP0fNfso6az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749f5331-b8a1-4d34-c6e1-3a091ee8e567"
      },
      "source": [
        "!tf_upgrade_v2 \\\n",
        "    --intree GIKT/ \\\n",
        "    --outtree GIKT_v2/ \\\n",
        "    --reportfile tree_report.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO line 10:17: Renamed 'tf.ConfigProto' to 'tf.compat.v1.ConfigProto'\n",
            "INFO line 13:9: Renamed 'tf.Session' to 'tf.compat.v1.Session'\n",
            "INFO line 19:16: Renamed 'tf.train.Saver' to 'tf.compat.v1.train.Saver'\n",
            "INFO line 25:21: Renamed 'tf.global_variables_initializer' to 'tf.compat.v1.global_variables_initializer'\n",
            "WARNING line 158:4: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n",
            "INFO line 48:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "WARNING line 49:27: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 49:27: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 50:56: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 51:24: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 51:24: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 51:70: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 51:70: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "INFO line 55:24: Added keywords to args of function 'tf.reduce_mean'\n",
            "INFO line 61:17: Changing keep_prob arg of tf.nn.dropout to rate\n",
            "\n",
            "INFO line 75:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "WARNING line 76:27: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 76:27: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 77:60: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 78:24: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 78:24: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 78:70: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 78:70: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "INFO line 82:24: Added keywords to args of function 'tf.reduce_mean'\n",
            "INFO line 89:17: Changing keep_prob arg of tf.nn.dropout to rate\n",
            "\n",
            "INFO line 29:25: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 30:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 31:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 32:37: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 33:30: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 34:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 35:35: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 36:26: Added keywords to args of function 'tf.shape'\n",
            "WARNING line 37:33: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 37:33: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 37:130: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "INFO line 56:41: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 56:41: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 57:40: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 57:40: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 59:38: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 59:38: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 60:37: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 60:37: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 63:34: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 63:34: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 75:50: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 76:47: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 80:50: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 81:47: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 100:43: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 107:25: Renamed 'tf.contrib.rnn.BasicLSTMCell' to 'tf.compat.v1.nn.rnn_cell.BasicLSTMCell'\n",
            "WARNING line 108:27: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n",
            "ERROR line 108:27: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "INFO line 111:27: Renamed 'tf.contrib.rnn.MultiRNNCell' to 'tf.compat.v1.nn.rnn_cell.MultiRNNCell'\n",
            "INFO line 171:21: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 185:21: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 196:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "WARNING line 197:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 197:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 197:79: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 198:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 198:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 198:78: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 199:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 199:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 199:62: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 199:62: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "WARNING line 200:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 200:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 200:62: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 200:62: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "INFO line 218:22: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 227:16: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 230:20: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 238:25: Renamed 'tf.trainable_variables' to 'tf.compat.v1.trainable_variables'\n",
            "INFO line 239:47: Added keywords to args of function 'tf.gradients'\n",
            "INFO line 244:24: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 257:44: Renamed 'tf.batch_gather' to 'tf.compat.v1.batch_gather'\n",
            "INFO line 265:29: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 267:30: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 269:23: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 291:23: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 303:26: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 313:44: Renamed 'tf.batch_gather' to 'tf.compat.v1.batch_gather'\n",
            "INFO line 321:19: Added keywords to args of function 'tf.transpose'\n",
            "INFO line 322:19: Added keywords to args of function 'tf.transpose'\n",
            "INFO line 323:59: Added keywords to args of function 'tf.shape'\n",
            "INFO line 327:88: Added keywords to args of function 'tf.shape'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 5 files\n",
            "Detected 17 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "File: GIKT/model.py\n",
            "--------------------------------------------------------------------------------\n",
            "GIKT/model.py:37:33: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:56:41: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:57:40: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:59:38: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:60:37: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:63:34: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:108:27: WARNING: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n",
            "GIKT/model.py:108:27: ERROR: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "GIKT/model.py:197:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:198:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:199:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:200:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "--------------------------------------------------------------------------------\n",
            "File: GIKT/train.py\n",
            "--------------------------------------------------------------------------------\n",
            "GIKT/train.py:158:4: WARNING: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n",
            "--------------------------------------------------------------------------------\n",
            "File: GIKT/aggregators.py\n",
            "--------------------------------------------------------------------------------\n",
            "GIKT/aggregators.py:49:27: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/aggregators.py:51:24: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/aggregators.py:76:27: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/aggregators.py:78:24: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'tree_report.txt'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXEH8C4DNa1"
      },
      "source": [
        "#*Test* Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaJNfaUjhWp",
        "outputId": "ee2dcb87-0390-430b-99f6-7774f8347c1a"
      },
      "source": [
        "#/content/GIKT_v2/main.py \\\n",
        "!python /content/graph_kt/main.py --dataset /content/data/assist09_3/assist09_3 \\\n",
        "--n_hop 3 \\\n",
        "--log_dir /content/logs \\\n",
        "--checkpoint_dir /content/checkpoint \\\n",
        "--skill_neighbor_num 4 \\\n",
        "--question_neighbor_num 4 \\\n",
        "--hist_neighbor_num 3 \\\n",
        "--next_neighbor_num 4 \\\n",
        "--model hsei \\\n",
        "--lr 0.001 \\\n",
        "--att_bound 0.7 \\\n",
        "--sim_emb question_emb \\\n",
        "--dropout_keep_probs [0.8,0.8,1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-17 18:06:30.791642: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "hsei\n",
            "{'data_dir': 'data', 'log_dir': '/content/logs', 'train': 1, 'hidden_neurons': [200, 100], 'lr': 0.001, 'lr_decay': 0.92, 'checkpoint_dir': '/content/checkpoint', 'dropout_keep_probs': '[0.8,0.8,1]', 'aggregator': 'sum', 'model': 'hsei', 'l2_weight': 1e-08, 'limit_max_len': 200, 'limit_min_len': 3, 'dataset': '/content/data/assist09_3/assist09_3', 'field_size': 3, 'embedding_size': 100, 'max_step': 200, 'input_trans_size': 100, 'batch_size': 32, 'select_index': [0, 1, 2], 'num_epochs': 150, 'n_hop': 3, 'skill_neighbor_num': 4, 'question_neighbor_num': 4, 'hist_neighbor_num': 3, 'next_neighbor_num': 4, 'att_bound': 0.7, 'sim_emb': 'question_emb', 'tag': 1626545192.6224916}\n",
            "original test seqs num:893\n",
            "167\n",
            "17737\n",
            "2021-07-17 18:06:40.247804: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-17 18:06:40.248722: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-17 18:06:40.277228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-17 18:06:40.277817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-07-17 18:06:40.277864: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-17 18:06:40.280330: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-17 18:06:40.280429: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-07-17 18:06:40.282030: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-17 18:06:40.282367: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-17 18:06:40.284136: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-17 18:06:40.284943: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-07-17 18:06:40.285182: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-07-17 18:06:40.285302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-17 18:06:40.285894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-17 18:06:40.286410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-17 18:06:40.286467: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-17 18:06:40.773872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-17 18:06:40.773926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-07-17 18:06:40.773942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-07-17 18:06:40.774118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-17 18:06:40.774763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-17 18:06:40.775317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-17 18:06:40.775848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "hsei\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:708: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:987: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_8_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_8_grad/Reshape:0\", shape=(None, None, 100), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_8_grad/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_7_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/concat_2_grad/Slice_1:0\", shape=(None, None, None), dtype=float32), dense_shape=Tensor(\"gradients/concat_2_grad/Shape:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients_1/GatherV2_8_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients_1/GatherV2_8_grad/Reshape:0\", shape=(None, None, 100), dtype=float32), dense_shape=Tensor(\"gradients_1/GatherV2_8_grad/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients_1/GatherV2_7_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients_1/concat_2_grad/tuple/control_dependency:0\", shape=(None, None, None), dtype=float32), dense_shape=Tensor(\"gradients_1/concat_2_grad/Shape:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "initialize complete\n",
            "2021-07-17 18:07:00.410532: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000189999 Hz\n",
            "  0% 0/150 [00:00<?, ?it/s]epoch: 0\n",
            "/content/graph_kt/data_process.py:253: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  target_answers = pad_sequences(np.array([[j[-1] - feature_size for j in i[1:]] for i in seqs]), maxlen=max_step - 1,\n",
            "2021-07-17 18:07:08.694893: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-17 18:07:09.217026: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "  0% 0/150 [00:08<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1360, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
            "  (0) Invalid argument: ConcatOp : Dimensions of inputs should match: shape[0] = [32,199,1,100] vs. shape[1] = [6400,199,3,100]\n",
            "\t [[{{node concat_4}}]]\n",
            "\t [[Sum_2/_201]]\n",
            "  (1) Invalid argument: ConcatOp : Dimensions of inputs should match: shape[0] = [32,199,1,100] vs. shape[1] = [6400,199,3,100]\n",
            "\t [[{{node concat_4}}]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/graph_kt/main.py\", line 76, in <module>\n",
            "    main()\n",
            "  File \"/content/graph_kt/main.py\", line 69, in main\n",
            "    train(args,train_dkt)\n",
            "  File \"/content/graph_kt/train.py\", line 50, in train\n",
            "    binary_pred, pred, loss = model.train(sess,features_answer_index,target_answers,seq_lens,hist_neighbor_index)\n",
            "  File \"/content/graph_kt/model.py\", line 393, in train\n",
            "    [self.binary_pred, self.pred, self.loss, self.train_op, self.flat_target_correctness], input_feed)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1369, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
            "  (0) Invalid argument: ConcatOp : Dimensions of inputs should match: shape[0] = [32,199,1,100] vs. shape[1] = [6400,199,3,100]\n",
            "\t [[node concat_4 (defined at content/graph_kt/model.py:157) ]]\n",
            "\t [[Sum_2/_201]]\n",
            "  (1) Invalid argument: ConcatOp : Dimensions of inputs should match: shape[0] = [32,199,1,100] vs. shape[1] = [6400,199,3,100]\n",
            "\t [[node concat_4 (defined at content/graph_kt/model.py:157) ]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "Errors may have originated from an input operation.\n",
            "Input Source operations connected to node concat_4:\n",
            " Reshape_72 (defined at content/graph_kt/model.py:240)\n",
            "\n",
            "Input Source operations connected to node concat_4:\n",
            " Reshape_72 (defined at content/graph_kt/model.py:240)\n",
            "\n",
            "Original stack trace for 'concat_4':\n",
            "  File \"content/graph_kt/main.py\", line 76, in <module>\n",
            "    main()\n",
            "  File \"content/graph_kt/main.py\", line 69, in main\n",
            "    train(args,train_dkt)\n",
            "  File \"content/graph_kt/train.py\", line 18, in train\n",
            "    model = GIKT(args)\n",
            "  File \"content/graph_kt/model.py\", line 45, in __init__\n",
            "    self.build_model()\n",
            "  File \"content/graph_kt/model.py\", line 157, in build_model\n",
            "    Nh = tf.concat([tf.expand_dims(output_series, 2), self.hist_neighbors_features], 2)  # [self.batch_size,max_step,M+1,feature_trans_size]]\n",
            "  File \"usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\n",
            "    return target(*args, **kwargs)\n",
            "  File \"usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1768, in concat\n",
            "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
            "  File \"usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1228, in concat_v2\n",
            "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
            "  File \"usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\n",
            "    attrs=attr_protos, op_def=op_def)\n",
            "  File \"usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3565, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2045, in __init__\n",
            "    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW6VLcpUq1mG"
      },
      "source": [
        "#Modules unit testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7GXrI6cVIXm"
      },
      "source": [
        "#aggregators.py\n",
        "import tensorflow as tf\n",
        "from abc import abstractmethod\n",
        "\n",
        "LAYER_IDS = {}\n",
        "\n",
        "\n",
        "def get_layer_id(layer_name=''):\n",
        "    if layer_name not in LAYER_IDS:\n",
        "        LAYER_IDS[layer_name] = 0\n",
        "        return 0\n",
        "    else:\n",
        "        LAYER_IDS[layer_name] += 1\n",
        "        return LAYER_IDS[layer_name]\n",
        "\n",
        "\n",
        "class Aggregator(object):\n",
        "    def __init__(self, batch_size, seq_len,dim, dropout, act, name):\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_id(layer))\n",
        "        self.name = name\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.dim = dim\n",
        "\n",
        "    def __call__(self, self_vectors, neighbor_vectors, question_embeddings):\n",
        "        outputs = self._call(self_vectors, neighbor_vectors, question_embeddings)\n",
        "        return outputs\n",
        "\n",
        "    @abstractmethod\n",
        "    def _call(self, self_vectors, neighbor_vectors, question_embeddings):\n",
        "        # dimension:\n",
        "        # self_vectors: [batch_size, -1, dim]\n",
        "        # neighbor_vectors: [batch_size, -1, n_neighbor, dim]\n",
        "        # neighbor_relations: [batch_size, -1, n_neighbor, dim]\n",
        "        # user_embeddings: [batch_size, dim]\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SumAggregator(Aggregator):\n",
        "    def __init__(self, batch_size, seq_len,dim, dropout=0., act=tf.nn.relu, name=None):\n",
        "        super(SumAggregator, self).__init__(batch_size,seq_len,dim, dropout, act, name)\n",
        "\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.weights = tf.get_variable(\n",
        "                shape=[self.dim, self.dim], initializer=tf.contrib.layers.xavier_initializer(), name='weights')\n",
        "            self.bias = tf.get_variable(shape=[self.dim], initializer=tf.zeros_initializer(), name='bias')\n",
        "\n",
        "    def _call(self,self_vectors, neighbor_vectors, question_embeddings):\n",
        "        # [batch_size,seq_len, -1, dim]\n",
        "        neighbors_agg = tf.reduce_mean(neighbor_vectors, axis=-2)\n",
        "        output = tf.reshape(self_vectors + neighbors_agg, [-1, self.dim])\n",
        "        #neighbors_agg = tf.concat([tf.reshape(self_vectors,[self.batch_size,self.seq_len,-1,1,self.dim]),neighbor_vectors],-2)\n",
        "\n",
        "        # [-1, dim]\n",
        "        #output = tf.reshape(tf.reduce_mean(neighbors_agg,-2), [-1, self.dim])\n",
        "        #output = tf.nn.dropout(output, keep_prob=self.dropout)\n",
        "        output = tf.nn.dropout(output, rate=1 - keep_prob)\n",
        "        output = tf.matmul(output, self.weights) + self.bias\n",
        "\n",
        "        # [batch_size,seq_len, -1, dim]\n",
        "        output = tf.reshape(output, [self.batch_size,self.seq_len, -1, self.dim])\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n",
        "\n",
        "class ConcatAggregator(Aggregator):\n",
        "    def __init__(self, batch_size,seq_len, dim, dropout=0., act=tf.nn.relu, name=None):\n",
        "        super(ConcatAggregator, self).__init__(batch_size, seq_len,dim, dropout, act, name)\n",
        "\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.weights = tf.get_variable(\n",
        "                shape=[self.dim * 2, self.dim], initializer=tf.contrib.layers.xavier_initializer(), name='weights')\n",
        "            self.bias = tf.get_variable(shape=[self.dim], initializer=tf.zeros_initializer(), name='bias')\n",
        "\n",
        "    def _call(self, self_vectors, neighbor_vectors, question_embeddings):\n",
        "        # [batch_size,seq_len, -1, dim]\n",
        "        neighbors_agg = tf.reduce_mean(neighbor_vectors, axis=-2)\n",
        "\n",
        "        # [batch_size,seq_len, -1, dim * 2]\n",
        "        output = tf.concat([self_vectors, neighbors_agg], axis=-1)\n",
        "\n",
        "        # [-1, dim * 2]\n",
        "        output = tf.reshape(output, [-1, self.dim * 2])\n",
        "        #output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
        "        output = tf.nn.dropout(output, rate=keep_prob)\n",
        "\n",
        "        # [-1, dim]\n",
        "        output = tf.matmul(output, self.weights) + self.bias\n",
        "\n",
        "        # [batch_size, -1, dim]\n",
        "        output = tf.reshape(output, [self.batch_size,self.seq_len, -1, self.dim])\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4soS3_jc8tO"
      },
      "source": [
        "# model.py\n",
        "\n",
        "# encoding:utf-8\n",
        "import tensorflow as tf\n",
        "#from aggregators import SumAggregator, ConcatAggregator\n",
        "\n",
        "\n",
        "class GIKT(object):\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.args = args\n",
        "        self.hidden_neurons = args.hidden_neurons\n",
        "        self.max_step = args.max_step - 1\n",
        "        self.feature_answer_size = args.feature_answer_size\n",
        "        self.field_size = args.field_size\n",
        "        self.embedding_size = args.embedding_size\n",
        "\n",
        "        self.dropout_keep_probs = eval(args.dropout_keep_probs)\n",
        "        self.select_index = args.select_index\n",
        "        self.hist_neighbor_num = args.hist_neighbor_num  # M\n",
        "        self.next_neighbor_num = args.next_neighbor_num  # N\n",
        "        self.lr = args.lr\n",
        "        self.n_hop = args.n_hop\n",
        "\n",
        "        self.question_neighbor_num = args.question_neighbor_num\n",
        "        self.skill_neighbor_num = args.skill_neighbor_num\n",
        "\n",
        "        self.question_neighbors = args.question_neighbors\n",
        "        self.skill_neighbors = args.skill_neighbors\n",
        "\n",
        "        self.keep_prob = tf.compat.v1.placeholder(tf.float32)  # dropout keep prob\n",
        "        self.keep_prob_gnn = tf.compat.v1.placeholder(tf.float32)  # dropout keep prob\n",
        "        self.is_training = tf.compat.v1.placeholder(tf.bool)\n",
        "        self.features_answer_index = tf.compat.v1.placeholder(tf.int32, [None, self.max_step + 1, self.field_size])\n",
        "        self.target_answers = tf.compat.v1.placeholder(tf.float32, [None, self.max_step])\n",
        "        self.sequence_lens = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "        self.hist_neighbor_index = tf.compat.v1.placeholder(tf.int32, [None, self.max_step, self.hist_neighbor_num])\n",
        "        self.batch_size = tf.shape(self.features_answer_index)[0]\n",
        "        self.feature_embedding = tf.compat.v1.get_variable(\"feature_embedding\", [self.feature_answer_size, self.embedding_size],initializer=tf.keras.initializers.GlorotUniform())\n",
        "\n",
        "        if args.aggregator == 'sum':\n",
        "            self.aggregator_class = SumAggregator\n",
        "        elif args.aggregator == 'concat':\n",
        "            self.aggregator_class = ConcatAggregator\n",
        "        else:\n",
        "            raise Exception(\"Unknown aggregator: \" + args.aggregator)\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        hidden_size = self.hidden_neurons[-1]\n",
        "        select_feature_index = tf.gather(self.features_answer_index, self.select_index, axis=-1)\n",
        "        select_size = len(self.select_index)\n",
        "        questions_index = select_feature_index[:, :-1, 1]\n",
        "        next_questions_index =select_feature_index[:,1:,1]\n",
        "        skill_index = select_feature_index[:,:-1,0]\n",
        "        next_skill_index =select_feature_index[:,1:,0]\n",
        "\n",
        "        self.input_questions_embedding = tf.nn.embedding_lookup(self.feature_embedding, questions_index) #[batch_size,seq_len,d]\n",
        "        self.next_questions_embedding = tf.nn.embedding_lookup(self.feature_embedding,next_questions_index) #[batch_size,seq_len,select_size-1,d]\n",
        "\n",
        "        self.input_skills_embedding = tf.nn.embedding_lookup(self.feature_embedding, skill_index) #[batch_size,seq_len,d]\n",
        "        self.next_skills_embedding = tf.nn.embedding_lookup(self.feature_embedding,next_skill_index) #[batch_size,seq_len,select_size-1,d]\n",
        "\n",
        "\n",
        "        input_answers_embedding = tf.nn.embedding_lookup(self.feature_embedding,select_feature_index[:,:-1,-1]) #[batch_size,seq_len,1,d]\n",
        "        input_answers_index = select_feature_index[:,:-1,-1]\n",
        "\n",
        "        if self.n_hop>0:\n",
        "            #gnn\n",
        "            input_neighbors = self.get_neighbors(self.n_hop,questions_index)##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "            aggregate_embedding,self.aggregators = self.aggregate(input_neighbors, self.input_questions_embedding)\n",
        "\n",
        "            next_input_neighbors = self.get_neighbors(self.n_hop,next_questions_index)##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "            next_aggregate_embedding,self.aggregators = self.aggregate(next_input_neighbors, self.next_questions_embedding)\n",
        "\n",
        "            feature_emb_size =  self.embedding_size\n",
        "            feature_trans_embedding  = tf.reshape(tf.keras.layers.Dense(tf.reshape(aggregate_embedding[0],[-1,feature_emb_size]),hidden_size, activation = None, name = 'feature_layer', reuse = False), [-1,self.max_step, hidden_size]) \n",
        "            #feature_trans_embedding  = tf.reshape(tf.keras.layers.Dense(tf.reshape(aggregate_embedding[0],[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu, name = 'feature_layer', reuse = False), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "            next_trans_embedding  = tf.reshape(tf.keras.layers.Dense(tf.reshape(next_aggregate_embedding[0],[-1,feature_emb_size]),hidden_size, activation = 'relu', name = 'feature_layer', reuse = True), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "\n",
        "        else:\n",
        "            feature_emb_size =  self.embedding_size\n",
        "            feature_trans_embedding  = tf.reshape(tf.compat.v1.layers.dense(tf.reshape(self.input_questions_embedding,[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu(), name = 'feature_layer', reuse = False), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "            next_trans_embedding  = tf.reshape(tf.layers.dense(tf.reshape(self.next_questions_embedding,[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu(), name = 'feature_layer', reuse = True), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "\n",
        "            # # gnn\n",
        "            input_neighbors = self.get_neighbors(1,\n",
        "                                                 questions_index)  ##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "\n",
        "            next_input_neighbors = self.get_neighbors(1,\n",
        "                                                      next_questions_index)  ##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "\n",
        "            next_aggregate_embedding = [next_trans_embedding,tf.reshape(tf.gather(self.feature_embedding, tf.reshape(next_input_neighbors[-1], [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])]\n",
        "            aggregate_embedding = [feature_trans_embedding,tf.reshape(tf.gather(self.feature_embedding, tf.reshape(input_neighbors[-1], [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        input_fa_embedding = tf.reshape(tf.concat([feature_trans_embedding,input_answers_embedding],-1),[-1,hidden_size+self.embedding_size]) #embedding_size*2\n",
        "        input_trans_embedding = tf.reshape(tf.layers.dense(input_fa_embedding, hidden_size),\n",
        "                                            [-1, self.max_step, hidden_size])\n",
        "\n",
        "\n",
        "        #create rnn cell\n",
        "        hidden_layers = []\n",
        "        for idx, hidden_size in enumerate(self.hidden_neurons):\n",
        "            lstm_layer = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size,name='input_rnn%d'%idx)\n",
        "            hidden_layer = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell=lstm_layer,\n",
        "                                                         output_keep_prob=self.keep_prob)\n",
        "            hidden_layers.append(hidden_layer)\n",
        "        self.hidden_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells=hidden_layers, state_is_tuple=True)  # RNN\n",
        "\n",
        "        output_series = []\n",
        "        self.state = self.hidden_cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "        for i in range(self.max_step):\n",
        "            current_output, self.state = self.hidden_cell(input_trans_embedding[:, i, :], self.state)\n",
        "            output_series.append(current_output)\n",
        "\n",
        "        output_series = tf.reshape(tf.concat(output_series, 1), [-1, self.max_step, hidden_size])\n",
        "\n",
        "\n",
        "        if self.args.model == \"hssi\":\n",
        "            self.hist_neighbors_features = self.hist_neighbor_sampler(\n",
        "                output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "        elif self.args.model == \"hsei\":\n",
        "            self.hist_neighbors_features = self.hist_neighbor_sampler(\n",
        "                input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "        elif self.args.model == \"ssei\":\n",
        "            if self.args.sim_emb == \"skill_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_skills_embedding,\n",
        "                                                                           self.next_skills_embedding,\n",
        "                                                                           input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            elif self.args.sim_emb == \"question_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_questions_embedding,\n",
        "                                                                           self.next_questions_embedding,\n",
        "                                                                           input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            else:\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(feature_trans_embedding,\n",
        "                                                                           next_trans_embedding,\n",
        "                                                                           input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "        else:\n",
        "            if self.args.sim_emb == \"skill_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_skills_embedding,\n",
        "                                                                           self.next_skills_embedding,\n",
        "                                                                           output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            elif self.args.sim_emb == \"question_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_questions_embedding,\n",
        "                                                                           self.next_questions_embedding,\n",
        "                                                                           output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            else:\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(feature_trans_embedding,\n",
        "                                                                           next_trans_embedding,\n",
        "                                                                           output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "\n",
        "        if self.next_neighbor_num!=0:\n",
        "            Nn = self.next_neighbor_sampler(next_aggregate_embedding)  # [batch_size,max_step,N+1,embedding_size]\n",
        "            Nn = tf.concat([tf.expand_dims(next_trans_embedding,2),Nn],-2)\n",
        "            next_neighbor_num = self.next_neighbor_num+1\n",
        "        else:\n",
        "            Nn = tf.expand_dims(next_trans_embedding, 2)\n",
        "            next_neighbor_num = 1\n",
        "\n",
        "\n",
        "        if self.hist_neighbor_num != 0:\n",
        "\n",
        "\n",
        "            Nh = tf.concat([tf.expand_dims(output_series, 2), self.hist_neighbors_features],\n",
        "                           2)  # [self.batch_size,max_step,M+1,feature_trans_size]]\n",
        "\n",
        "            logits = tf.reduce_sum(tf.expand_dims(Nh, 3) * tf.expand_dims(Nn, 2),\n",
        "                                   axis=4)  # [-1,max_step,Nh,1,emb_size]*[-1,max_step,1,Nn,emb_size]\n",
        "\n",
        "\n",
        "\n",
        "            logits = tf.reshape(logits, [-1, self.max_step, (\n",
        "                        self.hist_neighbor_num + 1) * next_neighbor_num])  # ====>[batch_size,max_step,Nu*Nv]\n",
        "\n",
        "\n",
        "        else:\n",
        "\n",
        "            Nh = tf.expand_dims(output_series, 2)  # [self.batch_size,max_step,1,feature_trans_size]\n",
        "\n",
        "\n",
        "            logits = tf.reduce_sum(tf.expand_dims(Nh, 3) * tf.expand_dims(Nn, 2),\n",
        "                                   axis=4)  # [-1,max_step,Nh,1,emb_size]*[-1,max_step,1,Nn,emb_size]\n",
        "\n",
        "            logits = tf.reshape(logits,\n",
        "                                [-1, self.max_step, 1 * next_neighbor_num])  # ====>[batch_size,max_step,Nu*Nv]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope('ni'):\n",
        "            w1 = tf.get_variable('atn_weights_1',[hidden_size, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
        "            w2 = tf.get_variable('atn_weights_2',[hidden_size, 1],initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b1 = tf.get_variable('atn_bias_1',[1],initializer=tf.zeros_initializer())\n",
        "            b2 = tf.get_variable('atn_bias_2',[1],initializer=tf.zeros_initializer())\n",
        "        if select_size > 3:\n",
        "\n",
        "            f1 = tf.reshape(tf.matmul(tf.reshape(Nh, [-1, hidden_size]), w1) + b1,\n",
        "                            [-1, self.max_step, self.hist_neighbor_num + 1, 1])\n",
        "            f2 = tf.reshape(tf.matmul(tf.reshape(Nn, [-1, hidden_size]), w2) + b2,\n",
        "                            [-1, self.max_step, 1, next_neighbor_num])\n",
        "            coefs = tf.nn.softmax(tf.nn.tanh(\n",
        "                tf.reshape(f1 + f2, [-1, self.max_step, (self.hist_neighbor_num + 1) * next_neighbor_num])))  # temp=10\n",
        "        else:\n",
        "            f1 = tf.reshape(tf.matmul(tf.reshape(Nh, [-1, hidden_size]), w1) + b1,\n",
        "                            [-1, self.max_step, self.hist_neighbor_num + 1, 1])\n",
        "            f2 = tf.reshape(tf.matmul(tf.reshape(Nn, [-1, hidden_size]), w2) + b2,\n",
        "                            [-1, self.max_step, 1, next_neighbor_num])\n",
        "            coefs = tf.nn.softmax(tf.nn.tanh(\n",
        "                tf.reshape(f1 + f2, [-1, self.max_step, (self.hist_neighbor_num + 1) * next_neighbor_num])))  # temp=10\n",
        "\n",
        "        #coefs = tf.nn.softmax(logits)\n",
        "        self.logits = tf.reduce_sum(logits * coefs, axis=-1)\n",
        "\n",
        "        self.flat_target_logits = flat_target_logits = tf.reshape(self.logits, [-1])\n",
        "        self.flat_target_correctness = tf.reshape(self.target_answers, [-1])\n",
        "        self.pred = tf.sigmoid(tf.reshape(flat_target_logits, [-1, self.max_step]))\n",
        "        self.binary_pred = tf.cast(tf.greater_equal(self.pred, 0.5), tf.int32)\n",
        "\n",
        "        self.filling_seqs = tf.cast(tf.sequence_mask(self.sequence_lens - 1, self.max_step),\n",
        "                                    dtype=tf.float32)  # [batch_size,seq_len]\n",
        "        index = tf.where(tf.not_equal(tf.reshape(self.filling_seqs, [-1]), tf.constant(0, dtype=tf.float32)))\n",
        "        clear_flat_target_logits = tf.gather(self.flat_target_logits, index)\n",
        "        clear_flat_target_correctness = tf.gather(self.flat_target_correctness, index)\n",
        "        self.loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=clear_flat_target_correctness,\n",
        "                                                                          logits=clear_flat_target_logits))\n",
        "\n",
        "\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        # self.lr = tf.Variable(0.0, trainable=False)\n",
        "\n",
        "        trainable_vars = tf.trainable_variables()\n",
        "        self.grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars), 50)\n",
        "        # optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "        # optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "        # optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=0.95)\n",
        "        # self.train_op = optimizer.apply_gradients(zip(self.grads, trainable_vars))\n",
        "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
        "                                               beta1=0.9, beta2=0.999, epsilon=1e-8). \\\n",
        "            minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "        print(\"initialize complete\")\n",
        "\n",
        "\n",
        "\n",
        "    def hist_neighbor_sampler(self,input_embedding):\n",
        "        zero_embeddings = tf.expand_dims(tf.zeros([self.batch_size,self.hidden_neurons[-1]],dtype=tf.float32),1)#[batch_size,1,hidden_size]\n",
        "        input_embedding = tf.concat([input_embedding,zero_embeddings],1)#[batch_size,max_step+1,hidden_size]\n",
        "        #input_embedding:[batch_size,max_step,fa_trans_size]\n",
        "        temp_hist_index = tf.reshape(self.hist_neighbor_index,[-1,self.max_step*self.hist_neighbor_num]) #[self.batch_size, max_step*M]\n",
        "        hist_neighbors_features =tf.reshape(tf.batch_gather(input_embedding,temp_hist_index),[-1,self.max_step,self.hist_neighbor_num,input_embedding.shape[-1]])\n",
        "\n",
        "        #select last neigbor_num questions\n",
        "\n",
        "        return hist_neighbors_features\n",
        "\n",
        "    def hist_neighbor_sampler1(self, input_q_emb, next_q_emb, qa_emb):#sample based on question similarity\n",
        "        #next_q_emb:[batch_size,ms,emb_size]\n",
        "        mold_nextq = tf.sqrt(tf.reduce_sum(next_q_emb*next_q_emb,-1))#[bs,ms]\n",
        "        next_q_emb = tf.expand_dims(next_q_emb,2)\n",
        "        mold_inputq = tf.sqrt(tf.reduce_sum(input_q_emb*input_q_emb,-1))#[bs,ms]\n",
        "        input_q_emb = tf.expand_dims(input_q_emb,1)\n",
        "        q_similarity = tf.reduce_sum(next_q_emb*input_q_emb,-1)#[batch_size,ms,ms]\n",
        "        molds = tf.expand_dims(mold_nextq,2)*tf.expand_dims(mold_inputq,1)#[bs,ms,ms]\n",
        "        q_similarity = q_similarity/molds\n",
        "\n",
        "\n",
        "        zero_embeddings = tf.expand_dims(tf.zeros([self.batch_size,self.hidden_neurons[-1]],dtype=tf.float32),1)#[batch_size,1,hidden_size]\n",
        "        qa_emb = tf.concat([qa_emb, zero_embeddings], 1)#[batch_size,max_step+1,hidden_size]\n",
        "        paddings = tf.fill(value=-1,dims=[self.batch_size,self.hist_neighbor_num,self.hist_neighbor_num])\n",
        "\n",
        "\n",
        "        #mask future position\n",
        "        seq_mask = tf.range(1,self.max_step+1)\n",
        "        #input_qa_emb = tf.tile(tf.expand_dims(input_qa_emb,2),[1,1,self.max_step,1])\n",
        "        similarity_seqs = tf.tile(tf.expand_dims(tf.cast(tf.sequence_mask(seq_mask, self.max_step),\n",
        "                                    dtype=tf.float32),0),[self.batch_size,1,1])  # [batch_size,ms,ms]\n",
        "        #mask_seqs = tf.tile(tf.expand_dims(similarity_seqs,-1),[1,1,1,self.embedding_size])\n",
        "        #input_qa_emb = mask_seqs*input_qa_emb\n",
        "        q_similarity = q_similarity*similarity_seqs #only history q non zero# [batch_size,ms,ms]\n",
        "\n",
        "        #setting lower similarity bount\n",
        "        condition = tf.greater(q_similarity,self.args.att_bound)\n",
        "        #condition = tf.greater(q_similarity,0.9)\n",
        "        q_similarity = tf.where(condition,q_similarity,tf.zeros([self.batch_size,self.max_step,self.max_step]))\n",
        "        q_sim_index = tf.greater(q_similarity,0)#\n",
        "\n",
        "\n",
        "\n",
        "        self.q_similarity = q_similarity\n",
        "\n",
        "        temp_hist_index = tf.nn.top_k(q_similarity, self.hist_neighbor_num)[1]# [batch_size,ms,hist_num]\n",
        "        self.hist_attention_value = tf.nn.top_k(q_similarity, self.hist_neighbor_num)[0]# [batch_size,ms,hist_num]\n",
        "        #q_similarity[temp_hist_index]>0\n",
        "\n",
        "        #temp_hist_index = tf.where(self.hist_attention_value>self.args.att_bound,temp_hist_index,-1*tf.ones([self.batch_size,self.max_step,self.hist_neighbor_num],dtype=tf.int32))\n",
        "        temp_hist_index = tf.where(self.hist_attention_value>0,temp_hist_index,-1*tf.ones([self.batch_size,self.max_step,self.hist_neighbor_num],dtype=tf.int32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #temp_hist_index = tf.tile(tf.expand_dims(temp_hist_index[:,:,0],2),[1,1,self.hist_neighbor_num])\n",
        "        temp_hist_index = tf.reshape(temp_hist_index,[-1,self.max_step*self.hist_neighbor_num])\n",
        "        #self.temp_hist_index = tf.reshape(temp_hist_index, [-1, self.max_step, self.hist_neighbor_num])\n",
        "\n",
        "        hist_neighbors_features =tf.reshape(tf.batch_gather(qa_emb, temp_hist_index), [-1, self.max_step, self.hist_neighbor_num, qa_emb.shape[-1]])\n",
        "\n",
        "        return hist_neighbors_features\n",
        "\n",
        "\n",
        "    def next_neighbor_sampler(self,aggregate_embedding):\n",
        "\n",
        "        temp_emb = tf.reshape(aggregate_embedding[1],[-1,self.question_neighbor_num,self.embedding_size])\n",
        "        temp_emb = tf.transpose(temp_emb, [1, 0, 2])\n",
        "        temp_emb = tf.transpose(\n",
        "            tf.gather(temp_emb, tf.random.shuffle(tf.range(tf.shape(temp_emb)[0]))), [1, 0, 2])\n",
        "        if self.question_neighbor_num>=self.next_neighbor_num:\n",
        "            next_neighbors_embedding = tf.reshape(temp_emb[:,:self.next_neighbor_num,:],[self.batch_size,self.max_step,self.next_neighbor_num,self.embedding_size])\n",
        "        else:\n",
        "            tile_neighbor_embedding = tf.tile(temp_emb,[1, -(-self.next_neighbor_num // tf.shape(temp_emb)[0]), 1])\n",
        "            next_neighbors_embedding = tf.reshape(tile_neighbor_embedding[:,:self.next_neighbor_num,:],[self.batch_size,self.max_step,self.next_neighbor_num,self.embedding_size])\n",
        "\n",
        "        return next_neighbors_embedding\n",
        "\n",
        "    def get_neighbors(self,n_hop, question_index):\n",
        "        # question_index:[batch_size,seq_len]\n",
        "        # question_seed = tf.reshape(question_index#[batch_size*seq_len,1]\n",
        "        seeds = [question_index]  # [[batch_size,seq_len],[batch_size,seq_len,question_neighbor_num],batch_size,seq_len,question_neighbor_num,\n",
        "\n",
        "        for i in range(n_hop):\n",
        "            if i % 2 == 0:\n",
        "                neighbor = tf.reshape(tf.gather(self.question_neighbors, tf.reshape(seeds[i], [-1])),\n",
        "                                      [-1, self.max_step, self.question_neighbor_num])\n",
        "\n",
        "            else:\n",
        "                neighbor = tf.reshape(tf.gather(self.skill_neighbors, tf.reshape(seeds[i], [-1])),\n",
        "                                      [-1, self.max_step, self.skill_neighbor_num])\n",
        "\n",
        "\n",
        "            seeds.append(neighbor)  # [batch_size,seq_len,neighbor_num],[batch_size,seq_len,neighbor_num*neighbor_num]\n",
        "\n",
        "\n",
        "        return seeds\n",
        "\n",
        "    def aggregate(self, input_neighbors, input_questions_embedding):\n",
        "        # [[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]]\n",
        "        sq_neighbor_vectors = []\n",
        "        for hop_i, neighbors in enumerate(input_neighbors):\n",
        "            if hop_i % 2 == 0:  # question\n",
        "                temp_neighbors = tf.reshape(tf.gather(self.feature_embedding, tf.reshape(neighbors, [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])\n",
        "                sq_neighbor_vectors.append(temp_neighbors)\n",
        "            else:  # skill\n",
        "                temp_neighbors = tf.reshape(tf.gather(self.feature_embedding, tf.reshape(neighbors, [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])\n",
        "                sq_neighbor_vectors.append(temp_neighbors)\n",
        "        aggregators = []\n",
        "        for i in range(self.n_hop):\n",
        "            if i == self.n_hop - 1:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.max_step, self.embedding_size, act=tf.nn.tanh,\n",
        "                                                   dropout=self.keep_prob_gnn)\n",
        "            else:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.max_step, self.embedding_size, act=tf.nn.tanh,\n",
        "                                                   dropout=self.keep_prob_gnn)\n",
        "            aggregators.append(aggregator)\n",
        "\n",
        "            # vectors_next_iter = []\n",
        "            for hop in range(self.n_hop - i):  # aggregate from outside to inside#layer\n",
        "                if hop % 2 == 0:\n",
        "                    shape = [self.batch_size, self.max_step, -1, self.question_neighbor_num, self.embedding_size]\n",
        "                    vector = aggregator(self_vectors=sq_neighbor_vectors[hop],\n",
        "                                        neighbor_vectors=tf.reshape(sq_neighbor_vectors[hop + 1], shape),\n",
        "                                        question_embeddings=sq_neighbor_vectors[hop],\n",
        "                                        )  # [batch_size,seq_len, -1, dim]\n",
        "                else:\n",
        "                    shape = [self.batch_size, self.max_step, -1, self.skill_neighbor_num, self.embedding_size]\n",
        "                    vector = aggregator(self_vectors=sq_neighbor_vectors[hop],\n",
        "                                        neighbor_vectors=tf.reshape(sq_neighbor_vectors[hop + 1], shape),\n",
        "                                        question_embeddings=sq_neighbor_vectors[hop],\n",
        "                                        )  # [batch_size,seq_len, -1, dim]\n",
        "                # shape = [self.batch_size, self.max_step, -1, self.sample_neighbor_num, self.embedding_size]\n",
        "\n",
        "                # vectors_next_iter.append(vector)\n",
        "                sq_neighbor_vectors[hop] = vector\n",
        "            # sq_neighbor_vectors = vectors_next_iter\n",
        "\n",
        "        # res = tf.reshape(sq_neighbor_vectors[0], [self.batch_size,self.max_step, self.embedding_size])\n",
        "        res = sq_neighbor_vectors  # [[batch_size,max_step,-1,embedding_size]...]\n",
        "\n",
        "        return res, aggregators\n",
        "\n",
        "    # step on batch\n",
        "    def train(self, sess, features_answer_index, target_answers, seq_lens, hist_neighbor_index):\n",
        "\n",
        "        input_feed = {self.features_answer_index: features_answer_index,\n",
        "                      self.target_answers: target_answers,\n",
        "                      self.sequence_lens: seq_lens,\n",
        "                      self.hist_neighbor_index: hist_neighbor_index,\n",
        "                      self.is_training: True}\n",
        "\n",
        "        input_feed[self.keep_prob] = self.dropout_keep_probs[0]\n",
        "        input_feed[self.keep_prob_gnn] = self.dropout_keep_probs[1]\n",
        "        # input_feed[self.aggregate_keep_prob] = self.dropout_keep_probs[1]\n",
        "\n",
        "        bin_pred, pred, train_loss, _, aaaa = sess.run(\n",
        "            [self.binary_pred, self.pred, self.loss, self.train_op, self.flat_target_correctness], input_feed)\n",
        "\n",
        "\n",
        "        return bin_pred, pred, train_loss\n",
        "\n",
        "    def evaluate(self, sess, features_answer_index, target_answers, seq_lens, hist_neighbor_index,evaluate_step):\n",
        "\n",
        "        input_feed = {self.features_answer_index: features_answer_index,\n",
        "                      self.target_answers: target_answers,\n",
        "                      self.sequence_lens: seq_lens,\n",
        "                      self.hist_neighbor_index: hist_neighbor_index,\n",
        "                      self.is_training: False}\n",
        "\n",
        "        input_feed[self.keep_prob] = self.dropout_keep_probs[-1]\n",
        "        input_feed[self.keep_prob_gnn] = self.dropout_keep_probs[-1]\n",
        "\n",
        "        bin_pred, pred = sess.run([self.binary_pred, self.pred], input_feed)\n",
        "\n",
        "        return bin_pred, pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kbC02otv3Ki"
      },
      "source": [
        "#data_process.py\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def data_process(args):\n",
        "    # process data\n",
        "\n",
        "\n",
        "    train_data_directory = os.path.join(args.data_dir, args.dataset, args.dataset + '_train.csv')\n",
        "    valid_data_directory = os.path.join(args.data_dir, args.dataset, args.dataset + '_test.csv')\n",
        "    test_data_directory = os.path.join(args.data_dir, args.dataset, args.dataset + '_test.csv')\n",
        "    args.train_seqs,train_student_num,train_max_skill_id,train_max_question_id,feature_answer_id = load_data(train_data_directory,args.field_size,args.max_step)\n",
        "    args.test_seqs,test_student_num,test_max_skill_id,test_max_question_id,_ = load_data(test_data_directory,args.field_size,args.max_step)\n",
        "    args.valid_seqs,_,_,_,_ = load_data(valid_data_directory,args.field_size,args.max_step)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"original test seqs num:%d\"%len(args.test_seqs))\n",
        "    lens = []\n",
        "    for i in range(len(args.test_seqs)):\n",
        "        lens.append(len(args.test_seqs[i]))\n",
        "\n",
        "\n",
        "\n",
        "    lens = []\n",
        "    for i in range(len(args.test_seqs)):\n",
        "        lens.append(len(args.test_seqs[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    student_num = train_student_num+test_student_num\n",
        "    args.skill_num = max(train_max_skill_id,test_max_skill_id)+1\n",
        "    args.qs_num = max(train_max_question_id,test_max_question_id)+1\n",
        "    args.question_num = args.qs_num-args.skill_num\n",
        "    args.feature_answer_size = feature_answer_id+1\n",
        "    print(args.skill_num)\n",
        "    print(args.question_num)\n",
        "\n",
        "\n",
        "    matrix_directory = os.path.join(args.data_dir, args.dataset, args.dataset + '_skill_matrix.txt')\n",
        "    args.skill_matrix = np.loadtxt(matrix_directory) #multi-skill\n",
        "\n",
        "    qs_adj_list,interactions = build_adj_list(args.train_seqs,args.test_seqs,args.skill_matrix,args.qs_num)#[[neighbor skill/question] for all qs]]\n",
        "    args.question_neighbors,args.skill_neighbors = extract_qs_relations(qs_adj_list,args.skill_num,args.qs_num,args.question_neighbor_num,args.skill_neighbor_num)\n",
        "    # print(args.question_neighbors.shape)#the first s_num rows are 0\n",
        "    # print(args.skill_neighbors.shape)\n",
        "    # exit()\n",
        "    return args\n",
        "\n",
        "def select_part_seqs(min_len,max_len,seqs):\n",
        "    temp_seqs = []\n",
        "    for seq in seqs:\n",
        "        if len(seq)>=min_len and len(seq)<=max_len:\n",
        "            temp_seqs.append(seq)\n",
        "\n",
        "    print(\"seq num is: %d\"%len(temp_seqs))\n",
        "    return temp_seqs\n",
        "\n",
        "def build_adj_list(train_seqs,test_seqs,skill_matrix,qs_num):\n",
        "    #seqs:list-num_students,seq_len,field_size\n",
        "    #skill_matrix:[num_skill,num_skill]\n",
        "    #0:skill 1:question\n",
        "    #question-skill graph\n",
        "    interactions = 0\n",
        "    single_skill = []\n",
        "    adj_list = [[] for _ in range(qs_num)]\n",
        "    num_skill = skill_matrix.shape[0]\n",
        "\n",
        "    adj_num = [0 for _ in range(qs_num)]\n",
        "\n",
        "    for seqs in [train_seqs,test_seqs]:\n",
        "        for seq in seqs:\n",
        "            interactions+=len(seq)\n",
        "            for step in seq:\n",
        "                adj_list[step[1]] = np.reshape(np.argwhere(skill_matrix[step[0]] == 1),[-1]).tolist()\n",
        "                adj_num[step[1]] += 1\n",
        "                for skill_index in np.reshape(np.argwhere(skill_matrix[step[0]] == 1),[-1]).tolist():\n",
        "                    adj_num[skill_index] += 1\n",
        "                    if skill_index not in single_skill:\n",
        "                        single_skill.append(skill_index)\n",
        "                    if step[1] not in adj_list[skill_index]:\n",
        "                        adj_list[skill_index].append(step[1])\n",
        "\n",
        "\n",
        "    # print(\"average neighbor question num:{}\".format(np.sum([len(adj_list[i]) for i in range(num_skill)])/len(single_skill)))\n",
        "    # print(\"average neighbor skill num:{}\".format(np.sum([len(adj_list[i]) for i in range(num_skill,qs_num)])/(qs_num-num_skill)))\n",
        "\n",
        "\n",
        "    return adj_list,interactions\n",
        "\n",
        "def extract_qs_relations(qs_list,s_num,qs_num, q_neighbor_size, s_neighbor_size):\n",
        "    question_neighbors = np.zeros([qs_num, q_neighbor_size], dtype=np.int32)#the first s_num rows are 0\n",
        "    skill_neighbors = np.zeros([s_num, s_neighbor_size], dtype=np.int32)\n",
        "    s_num_dic = {}\n",
        "    q_num_dic = {}\n",
        "    for index,neighbors in enumerate(qs_list):\n",
        "        if index < s_num:  #s\n",
        "            if len(neighbors) not in q_num_dic:\n",
        "                q_num_dic[len(neighbors)] = 1\n",
        "            else:\n",
        "                q_num_dic[len(neighbors)] +=1\n",
        "            if len(neighbors) > 0:\n",
        "                if len(neighbors) >= s_neighbor_size:\n",
        "                    skill_neighbors[index] = np.random.choice(neighbors, s_neighbor_size, replace=False)\n",
        "                else:\n",
        "                    skill_neighbors[index] = np.random.choice(neighbors, s_neighbor_size, replace=True)\n",
        "        else:#q\n",
        "            #print(len(neighbors))\n",
        "            if len(neighbors) not in s_num_dic:\n",
        "                s_num_dic[len(neighbors)] = 1\n",
        "            else:\n",
        "                s_num_dic[len(neighbors)] +=1\n",
        "            if len(neighbors)>0:\n",
        "                if len(neighbors) >= q_neighbor_size:\n",
        "                    question_neighbors[index] = np.random.choice(neighbors, q_neighbor_size,replace=False)\n",
        "                else:\n",
        "                    question_neighbors[index] = np.random.choice(neighbors, q_neighbor_size, replace=True)\n",
        "\n",
        "    #q_num_dic = sorted(q_num_dic.items(), key=lambda d: d[1])\n",
        "    # print(s_num_dic)\n",
        "    # print(q_num_dic)\n",
        "    # exit()\n",
        "    return question_neighbors,skill_neighbors\n",
        "\n",
        "def load_data(dataset_path, field_size, max_seq_len):\n",
        "    seqs = []\n",
        "    student_id = 0\n",
        "    max_skill = -1\n",
        "    max_question = -1\n",
        "    feature_answer_size = -1\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        feature_answer_list = []\n",
        "        for lineid,line in enumerate(f):\n",
        "            fields = line.strip().strip(',')\n",
        "            i = lineid % (field_size+1)\n",
        "            if i != 0:#i==0 new student==>student seq len\n",
        "                feature_answer_list.append(list(map(int, fields.split(\",\"))))  #\n",
        "            if i == 1:\n",
        "                if max(feature_answer_list[-1]) > max_skill:\n",
        "                    max_skill = max(feature_answer_list[-1])\n",
        "            elif i == 2:\n",
        "                if max(feature_answer_list[-1])>max_question:\n",
        "                    max_question = max(feature_answer_list[-1])\n",
        "            elif i == field_size:\n",
        "                student_id += 1\n",
        "                if max(feature_answer_list[-1])>feature_answer_size:\n",
        "                    feature_answer_size = max(feature_answer_list[-1])\n",
        "                if len(feature_answer_list[0]) > max_seq_len:\n",
        "                    n_split = len(feature_answer_list[0]) // max_seq_len\n",
        "                    if len(feature_answer_list[0]) % max_seq_len:\n",
        "                        n_split += 1\n",
        "                else:\n",
        "                    n_split = 1\n",
        "                for k in range(n_split):\n",
        "                    # Less than 'seq_len' element remained\n",
        "                    if k == n_split - 1:\n",
        "                        end_index = len(feature_answer_list[0])\n",
        "                    else:\n",
        "                        end_index = (k + 1) * max_seq_len\n",
        "                    split_list = []\n",
        "\n",
        "                    for i in range(len(feature_answer_list)):\n",
        "                        split_list.append(feature_answer_list[i][k * max_seq_len:end_index])\n",
        "                        #if i == len(feature_answer_list)-2:#before answer\n",
        "                            #split_list.append([student_id]*(end_index-k*args.seq_len)) #student id\n",
        "\n",
        "                    split_list = np.stack(split_list,1).tolist()#[seq_len,field_size]\n",
        "\n",
        "                    seqs.append(split_list)\n",
        "                feature_answer_list = []\n",
        "\n",
        "    return seqs,student_id,max_skill,max_question,feature_answer_size\n",
        "\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
        "    lengths = [len(s) for s in sequences]\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "   # print(np.shape(sequences))\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if len(s) == 0:\n",
        "            continue  # empty list was found\n",
        "        if truncating == 'pre': # maxlen!=none may need to truncating\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen+1]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "#select same skill index\n",
        "def sample_hist_neighbors(seqs_size,max_step,hist_num,skill_index):\n",
        "    #skill_index:[batch_size,max_step]\n",
        "\n",
        "    #[batch_size,max_step,M]\n",
        "    hist_neighbors_index = []\n",
        "\n",
        "    for i in range(seqs_size):\n",
        "        seq_hist_index = []\n",
        "        seq_skill_index = skill_index[i]\n",
        "        #[max_step,M]\n",
        "        for j in range(1,max_step):\n",
        "            same_skill_index = [k for k in range(j) if seq_skill_index[k] == seq_skill_index[j]]\n",
        "\n",
        "            if hist_num != 0:\n",
        "                #[0,j] select M\n",
        "                if len(same_skill_index) >= hist_num:\n",
        "                    seq_hist_index.append(np.random.choice(same_skill_index,hist_num, replace=False))\n",
        "                else:\n",
        "                    if len(same_skill_index)!= 0:\n",
        "                        seq_hist_index.append(np.random.choice(same_skill_index,hist_num, replace=True))\n",
        "                    else:\n",
        "                        seq_hist_index.append(([max_step-1 for _ in range(hist_num)]))\n",
        "            else:\n",
        "                seq_hist_index.append([])\n",
        "        hist_neighbors_index.append(seq_hist_index)\n",
        "    return hist_neighbors_index\n",
        "\n",
        "\n",
        "def format_data(seqs, max_step, feature_size, hist_num):\n",
        "\n",
        "    seqs = seqs\n",
        "    seq_lens = np.array(list(map(lambda seq: len(seq), seqs)))\n",
        "\n",
        "    #[batch_size,max_len,feature_size]\n",
        "    features_answer_index = pad_sequences(seqs,maxlen=max_step, padding='post', value=0)\n",
        "    target_answers = pad_sequences(np.array([[j[-1] - feature_size for j in i[1:]] for i in seqs]), maxlen=max_step-1, padding='post', value=0)\n",
        "    skills_index = features_answer_index[:,:,0]\n",
        "    hist_neighbor_index = sample_hist_neighbors(len(seqs),max_step,hist_num,skills_index)#[batch_size,max_step,M]\n",
        "\n",
        "\n",
        "    return features_answer_index,target_answers,seq_lens,hist_neighbor_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "\n",
        "    def __init__(self, seqs, max_step, batch_size, feature_size, hist_num):#feature_dkt\n",
        "        np.random.seed(42)\n",
        "        self.seqs = seqs\n",
        "        self.max_step = max_step\n",
        "        self.batch_size = batch_size\n",
        "        self.batch_i = 0\n",
        "        self.end = False\n",
        "        self.feature_size = feature_size\n",
        "        self.n_batch = int(np.ceil(len(seqs) / batch_size))\n",
        "        self.hist_num = hist_num\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "\n",
        "        batch_seqs = self.seqs[self.batch_i*self.batch_size:(self.batch_i+1)*self.batch_size]\n",
        "        self.batch_i+=1\n",
        "\n",
        "        if self.batch_i == self.n_batch:\n",
        "            self.end = True\n",
        "\n",
        "        format_data_list = format_data(batch_seqs, self.max_step,self.feature_size,self.hist_num)  # [feature_index,target_answers,sequences_lens,hist_neighbor_index]\n",
        "        return format_data_list\n",
        "\n",
        "    def shuffle(self):\n",
        "        self.pos = 0\n",
        "        self.end = False\n",
        "        np.random.shuffle(self.seqs)\n",
        "\n",
        "    def reset(self):\n",
        "        self.pos = 0\n",
        "        self.end = False\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     seqs = [[[1,0],[2,0],[3,1]],[[2,0],[4,1]]]\n",
        "#     input_x, target_id, target_correctness, seq_len, max_len = format_data(seqs,1,5)\n",
        "#     print(input_x)\n",
        "#     print(target_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW_2lcLbM1Eg"
      },
      "source": [
        "#train.py\n",
        "\n",
        "#from model import GIKT\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
        "#from data_process import DataGenerator\n",
        "\n",
        "def train(args,train_dkt):\n",
        "    run_config = tf.compat.v1.ConfigProto()\n",
        "    run_config.gpu_options.allow_growth = True\n",
        "\n",
        "    with tf.compat.v1.Session(config=run_config) as sess:\n",
        "\n",
        "        print(args.model)\n",
        "\n",
        "        model = GIKT(args)\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        index = 0\n",
        "\n",
        "        if train_dkt:\n",
        "            # lr = 0.4\n",
        "            # lr_decay = 0.92\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "            model_dir = save_model_dir(args)\n",
        "\n",
        "            best_valid_auc = 0\n",
        "\n",
        "            for epoch in tqdm(range(args.num_epochs)):\n",
        "                train_generator = DataGenerator(args.train_seqs, args.max_step, batch_size=args.batch_size,\n",
        "                                                feature_size=args.feature_answer_size - 2,\n",
        "                                                hist_num=args.hist_neighbor_num)\n",
        "                valid_generator = DataGenerator(args.valid_seqs, args.max_step, batch_size=args.batch_size,\n",
        "                                                feature_size=args.feature_answer_size - 2,\n",
        "                                                hist_num=args.hist_neighbor_num)\n",
        "                #    assign_lr()\n",
        "                print(\"epoch:\", epoch)\n",
        "                # self.assign_lr(self.sess,self.args.lr * self.args.lr_decay ** epoch)\n",
        "                overall_loss = 0\n",
        "                train_generator.shuffle()\n",
        "                preds, binary_preds, targets = list(), list(), list()\n",
        "                train_step = 0\n",
        "                while not train_generator.end:\n",
        "                    train_step += 1\n",
        "\n",
        "                    [features_answer_index,target_answers,seq_lens,hist_neighbor_index] = train_generator.next_batch()\n",
        "                    binary_pred, pred, loss = model.train(sess,features_answer_index,target_answers,seq_lens,hist_neighbor_index)\n",
        "\n",
        "                    overall_loss += loss\n",
        "                    for seq_idx, seq_len in enumerate(seq_lens):\n",
        "                        preds.append(pred[seq_idx, 0:seq_len])\n",
        "                        binary_preds.append(binary_pred[seq_idx, 0:seq_len])\n",
        "                        targets.append(target_answers[seq_idx, 0:seq_len])\n",
        "                # print(\"\\r idx:{0}, overall_loss:{1}\".format(train_generator.pos, overall_loss)),\n",
        "                train_loss = overall_loss / train_step\n",
        "                preds = np.concatenate(preds)\n",
        "                binary_preds = np.concatenate(binary_preds)\n",
        "                targets = np.concatenate(targets)\n",
        "                auc_value = roc_auc_score(targets, preds)\n",
        "                accuracy = accuracy_score(targets, binary_preds)\n",
        "                precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
        "                print(\"\\ntrain loss = {0},auc={1}, accuracy={2}\".format(train_loss, auc_value, accuracy))\n",
        "                write_log(args,model_dir,auc_value, accuracy, epoch, name='train_')\n",
        "\n",
        "                # if epoch == self.args.num_epochs-1:\n",
        "                #     self.save(epoch)\n",
        "\n",
        "                # valid\n",
        "                valid_generator.reset()\n",
        "                preds, binary_preds, targets = list(), list(), list()\n",
        "                valid_step = 0\n",
        "                #overall_loss = 0\n",
        "                while not valid_generator.end:\n",
        "                    valid_step += 1\n",
        "                    [features_answer_index,target_answers,seq_lens,hist_neighbor_index] = valid_generator.next_batch()\n",
        "                    binary_pred, pred = model.evaluate(sess,features_answer_index,target_answers,seq_lens,hist_neighbor_index,valid_step)\n",
        "                    #overall_loss += loss\n",
        "                    for seq_idx, seq_len in enumerate(seq_lens):\n",
        "                        preds.append(pred[seq_idx, 0:seq_len])\n",
        "                        binary_preds.append(binary_pred[seq_idx, 0:seq_len])\n",
        "                        targets.append(target_answers[seq_idx, 0:seq_len])\n",
        "                # compute metrics\n",
        "                #valid_loss = overall_loss / valid_step\n",
        "                preds = np.concatenate(preds)\n",
        "                binary_preds = np.concatenate(binary_preds)\n",
        "                targets = np.concatenate(targets)\n",
        "                auc_value = roc_auc_score(targets, preds)\n",
        "                accuracy = accuracy_score(targets, binary_preds)\n",
        "                precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
        "                print(\"\\nvalid auc={0}, accuracy={1}, precision={2}, recall={3}\".format(auc_value, accuracy, precision,\n",
        "                                                                                        recall))\n",
        "\n",
        "\n",
        "                write_log(args,model_dir,auc_value, accuracy, epoch, name='valid_')\n",
        "\n",
        "                if auc_value > best_valid_auc:\n",
        "                    print('%3.4f to %3.4f' % (best_valid_auc, auc_value))\n",
        "                    best_valid_auc = auc_value\n",
        "                    best_epoch = epoch\n",
        "                    #np.save('feature_embedding.npy', feature_embedding)\n",
        "                    checkpoint_dir = os.path.join(args.checkpoint_dir, model_dir)\n",
        "                    save(best_epoch,sess,checkpoint_dir,saver)\n",
        "#                print(model_dir)\n",
        "                print(model_dir+\"\\t\"+str(best_valid_auc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "            if self.load():\n",
        "                print('CKPT loaded')\n",
        "            else:\n",
        "                raise Exception('CKPT need')\n",
        "            test_data_generator = DataGenerator(args.test_seqs, args.max_step, batch_size=args.batch_size,\n",
        "                                                feature_size=args.feature_answer_size - 2,\n",
        "                                                hist_num=args.hist_neighbor_num)\n",
        "            data_generator.reset()\n",
        "\n",
        "            correct_times = np.zeros(self.num_skills + 1)\n",
        "            preds, binary_preds, targets = list(), list(), list()\n",
        "            while not test_data_generator.end:\n",
        "\n",
        "                [features_answer_index, target_answers, seq_lens, hist_neighbor_index] = valid_generator.next_batch()\n",
        "                binary_pred, pred = model.evaluate(sess, features_answer_index, target_answers, seq_lens,\n",
        "                                                   hist_neighbor_index)\n",
        "                # overall_loss += loss\n",
        "                for seq_idx, seq_len in enumerate(seq_lens):\n",
        "                    preds.append(pred[seq_idx, 0:seq_len])\n",
        "                    binary_preds.append(binary_pred[seq_idx, 0:seq_len])\n",
        "                    targets.append(target_answers[seq_idx, 0:seq_len])\n",
        "\n",
        "\n",
        "\n",
        "            preds = np.concatenate(preds)\n",
        "            binary_preds = np.concatenate(binary_preds)\n",
        "            targets = np.concatenate(targets)\n",
        "            auc_value = roc_auc_score(targets, preds)\n",
        "            accuracy = accuracy_score(targets, binary_preds)\n",
        "            precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
        "            print(\"\\ntest auc={0}, accuracy={1}, precision={2}, recall={3}\".format(auc_value, accuracy, precision,\n",
        "                                                                                   recall))\n",
        "            print(model_dir)\n",
        "            write_log(args, model_dir, auc_value, accuracy, epoch, name='test_')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save(global_step,sess,checkpoint_dir,saver):\n",
        "    model_name = 'GIKT'\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.mkdir(checkpoint_dir)\n",
        "    saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=global_step)\n",
        "    print('Save checkpoint at %d' % (global_step))\n",
        "\n",
        "\n",
        "def save_model_dir(args):\n",
        "    return '{}_{}_{}lr_{}hop_{}sn_{}qn_{}hn_{}nn_{}_{}bound_{}keep_{}'.format(args.dataset,\n",
        "                                                args.model,args.lr,args.n_hop,args.skill_neighbor_num,args.question_neighbor_num,args.hist_neighbor_num,\\\n",
        "                                                                     args.next_neighbor_num,args.sim_emb,args.att_bound,args.dropout_keep_probs,args.tag)\n",
        "\n",
        "\n",
        "\n",
        "def write_log(args,model_dir,auc, accuracy, epoch, name='train_'):\n",
        "    log_path = os.path.join(args.log_dir, name+model_dir+'.csv')\n",
        "    if not os.path.exists(log_path):\n",
        "        log_file = open(log_path, 'w')\n",
        "        log_file.write('Epoch\\tAuc\\tAccuracy\\n')\n",
        "    else:\n",
        "        log_file = open(log_path, 'a')\n",
        "\n",
        "    log_file.write(str(epoch) + '\\t' + str(auc) + '\\t' + str(accuracy)  + '\\n')\n",
        "    log_file.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "NScWQp990I0v",
        "outputId": "03358ac2-a7b5-4d46-f69b-c8e9f166373a"
      },
      "source": [
        "#main.py\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy.distutils.fcompiler import str2bool\n",
        "#from data_process import *\n",
        "#from train import train\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "    train_dkt = 1\n",
        "    arg_parser = argparse.ArgumentParser(description=\"train dkt model\")\n",
        "    arg_parser.add_argument('--data_dir', type=str, default='data')\n",
        "    arg_parser.add_argument(\"--log_dir\", type=str, default='logs')\n",
        "    arg_parser.add_argument('--train', type=str2bool, default='t')\n",
        "    arg_parser.add_argument('--hidden_neurons', type=int, default=[200,100])\n",
        "    arg_parser.add_argument(\"--lr\", type=float, default=0.001)\n",
        "    arg_parser.add_argument(\"--lr_decay\", type=float, default=0.92)\n",
        "    arg_parser.add_argument('--checkpoint_dir', type=str, default='checkpoint')\n",
        "    arg_parser.add_argument('--dropout_keep_probs', nargs='?', default=[0.6,0.8,1])\n",
        "    arg_parser.add_argument('--aggregator', type=str, default='sum')\n",
        "    arg_parser.add_argument('--model', type=str, default='dkt')\n",
        "    arg_parser.add_argument('--l2_weight', type=float, default=1e-8)\n",
        "    arg_parser.add_argument('--limit_max_len',type=int,default=200)\n",
        "    arg_parser.add_argument('--limit_min_len',type=int,default=3)\n",
        "\n",
        "\n",
        "    arg_parser.add_argument('--dataset', type=str, default='ednet_5000_3')\n",
        "\n",
        "    arg_parser.add_argument(\"--field_size\", type=int, default=3)\n",
        "    arg_parser.add_argument(\"--embedding_size\", type=int, default=100)\n",
        "    arg_parser.add_argument(\"--max_step\", type=int, default=200)\n",
        "    arg_parser.add_argument(\"--input_trans_size\", type=int, default=100)\n",
        "    arg_parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    arg_parser.add_argument(\"--select_index\", type=int, default=[0,1, 2])\n",
        "    arg_parser.add_argument('--num_epochs', type=int, default=150)\n",
        "    arg_parser.add_argument('--n_hop', type=int, default=3)\n",
        "    arg_parser.add_argument('--skill_neighbor_num', type=int, default=10)\n",
        "    arg_parser.add_argument('--question_neighbor_num', type=int, default=4)\n",
        "    arg_parser.add_argument('--hist_neighbor_num', type=int, default=0)  # history neighbor num\n",
        "    arg_parser.add_argument('--next_neighbor_num', type=int, default=4)  # next neighbor num\n",
        "\n",
        "    arg_parser.add_argument('--att_bound', type=float, default=0.5)#filtring irralate emb in topk selection\n",
        "    arg_parser.add_argument('--sim_emb', type=str, default='skill_emb')#filtring irralate emb in topk selection\n",
        "\n",
        "\n",
        "\n",
        "    args = arg_parser.parse_args()\n",
        "    #args.dataset = dataset\n",
        "    print(args.model)\n",
        "    tag_path = os.path.join(\"%s_tag.txt\"%args.dataset)\n",
        "    tag = time.time()\n",
        "    args.tag = tag\n",
        "\n",
        "    config_name = 'logs/%f_config.json' % tag\n",
        "    config = {}\n",
        "    for k,v in vars(args).items():\n",
        "        config[k] = vars(args)[k]\n",
        "\n",
        "    jsObj = json.dumps(config)\n",
        "\n",
        "    fileObject = open(config_name, 'w')\n",
        "    fileObject.write(jsObj)\n",
        "    fileObject.close()\n",
        "    print(config)\n",
        "    args = data_process(args)\n",
        "\n",
        "    train(args,train_dkt)\n",
        "\n",
        "    log_file = open(tag_path, 'w')\n",
        "    log_file.write(str(tag))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR] [--log_dir LOG_DIR]\n",
            "                             [--train TRAIN] [--hidden_neurons HIDDEN_NEURONS]\n",
            "                             [--lr LR] [--lr_decay LR_DECAY]\n",
            "                             [--checkpoint_dir CHECKPOINT_DIR]\n",
            "                             [--dropout_keep_probs [DROPOUT_KEEP_PROBS]]\n",
            "                             [--aggregator AGGREGATOR] [--model MODEL]\n",
            "                             [--l2_weight L2_WEIGHT]\n",
            "                             [--limit_max_len LIMIT_MAX_LEN]\n",
            "                             [--limit_min_len LIMIT_MIN_LEN]\n",
            "                             [--dataset DATASET] [--field_size FIELD_SIZE]\n",
            "                             [--embedding_size EMBEDDING_SIZE]\n",
            "                             [--max_step MAX_STEP]\n",
            "                             [--input_trans_size INPUT_TRANS_SIZE]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "                             [--select_index SELECT_INDEX]\n",
            "                             [--num_epochs NUM_EPOCHS] [--n_hop N_HOP]\n",
            "                             [--skill_neighbor_num SKILL_NEIGHBOR_NUM]\n",
            "                             [--question_neighbor_num QUESTION_NEIGHBOR_NUM]\n",
            "                             [--hist_neighbor_num HIST_NEIGHBOR_NUM]\n",
            "                             [--next_neighbor_num NEXT_NEIGHBOR_NUM]\n",
            "                             [--att_bound ATT_BOUND] [--sim_emb SIM_EMB]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-5c3c8c15-3306-4905-b976-9ff35844b63d.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
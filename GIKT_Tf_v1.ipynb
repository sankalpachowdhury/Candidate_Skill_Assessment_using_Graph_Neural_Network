{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIKT_Tf_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoOhFdvFd1Jd23HyM17bZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankalpachowdhury/Candidate_Skill_Assessment_using_Graph_Neural_Network/blob/main/GIKT_Tf_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvn4T5fdq-K_"
      },
      "source": [
        "# Install Framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "Ibqsb6mmzdIZ",
        "outputId": "77902f9a-f6b2-4b05-e025-437cf378a7dd"
      },
      "source": [
        "# Run only if needed\n",
        "!pip install tensorflow==1.13.0rc1\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/80/18adfb46ba0a4044e9feaa0897ceae4673ac07d34deeb74490bc0d4e4987/tensorflow-1.13.0rc1-cp37-cp37m-manylinux1_x86_64.whl (92.7MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.34.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.1.2)\n",
            "Collecting tensorboard<1.13.0,>=1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 18.3MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (3.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.0rc1) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.0rc1) (3.1.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (4.6.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.0rc1) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.7.4.3)\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.13.0rc1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, keras-applications, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.12.2 tensorflow-1.13.0rc1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsZWoUyB35L4",
        "outputId": "5cf82827-25b4-4507-b4b5-1c422732a0bd"
      },
      "source": [
        "# run to check version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.13.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7mgqs0M2HVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb63aa4-b5f4-44c5-f822-7b04b57a657a"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8RM6kIXjlBc",
        "outputId": "0b5c06c2-990f-4ce9-9d27-b89457845bf5"
      },
      "source": [
        "# import all codes\n",
        "!git clone \"https://github.com/Rimoku/GIKT.git\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GIKT'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 19 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC7TzZFRrZEY"
      },
      "source": [
        "\n",
        "# import codes from own github "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9l1hmiVp0xu"
      },
      "source": [
        "#DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FhweX93p27O",
        "outputId": "aba26717-4e87-49a1-94e4-de0757d53731"
      },
      "source": [
        "#get the data from the gdrive\n",
        "\n",
        "# 1 mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDLIwwTsXSQ"
      },
      "source": [
        "# 2 copy the folders to local\n",
        "!cp -r /content/gdrive/MyDrive/FinalYear_Project_Dataset/data /content/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXnz4LCHqUlu"
      },
      "source": [
        "# create logs and checkpoint folders\n",
        "!mkdir logs\n",
        "!mkdir checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttNQQN3qdEX"
      },
      "source": [
        "#Convert to Tf v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09CNfHmmpvJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d84941-808e-4894-f654-5b853a282141"
      },
      "source": [
        "!tf_upgrade_v2 -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "usage: tf_upgrade_v2 [-h] [--infile INPUT_FILE] [--outfile OUTPUT_FILE]\n",
            "                     [--intree INPUT_TREE] [--outtree OUTPUT_TREE]\n",
            "                     [--copyotherfiles COPY_OTHER_FILES]\n",
            "                     [--reportfile REPORT_FILENAME]\n",
            "\n",
            "Convert a TensorFlow Python file to 2.0\n",
            "\n",
            "Simple usage:\n",
            "  tf_upgrade_v2.py --infile foo.py --outfile bar.py\n",
            "  tf_upgrade_v2.py --intree ~/code/old --outtree ~/code/new\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --infile INPUT_FILE   If converting a single file, the name of the file to\n",
            "                        convert\n",
            "  --outfile OUTPUT_FILE\n",
            "                        If converting a single file, the output filename.\n",
            "  --intree INPUT_TREE   If converting a whole tree of files, the directory to\n",
            "                        read from (relative or absolute).\n",
            "  --outtree OUTPUT_TREE\n",
            "                        If converting a whole tree of files, the output\n",
            "                        directory (relative or absolute).\n",
            "  --copyotherfiles COPY_OTHER_FILES\n",
            "                        If converting a whole tree of files, whether to copy\n",
            "                        the other files.\n",
            "  --reportfile REPORT_FILENAME\n",
            "                        The name of the file where the report log is\n",
            "                        stored.(default: report.txt)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHP0fNfso6az",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749f5331-b8a1-4d34-c6e1-3a091ee8e567"
      },
      "source": [
        "!tf_upgrade_v2 \\\n",
        "    --intree GIKT/ \\\n",
        "    --outtree GIKT_v2/ \\\n",
        "    --reportfile tree_report.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO line 10:17: Renamed 'tf.ConfigProto' to 'tf.compat.v1.ConfigProto'\n",
            "INFO line 13:9: Renamed 'tf.Session' to 'tf.compat.v1.Session'\n",
            "INFO line 19:16: Renamed 'tf.train.Saver' to 'tf.compat.v1.train.Saver'\n",
            "INFO line 25:21: Renamed 'tf.global_variables_initializer' to 'tf.compat.v1.global_variables_initializer'\n",
            "WARNING line 158:4: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n",
            "INFO line 48:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "WARNING line 49:27: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 49:27: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 50:56: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 51:24: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 51:24: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 51:70: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 51:70: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "INFO line 55:24: Added keywords to args of function 'tf.reduce_mean'\n",
            "INFO line 61:17: Changing keep_prob arg of tf.nn.dropout to rate\n",
            "\n",
            "INFO line 75:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "WARNING line 76:27: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 76:27: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 77:60: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 78:24: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 78:24: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 78:70: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 78:70: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "INFO line 82:24: Added keywords to args of function 'tf.reduce_mean'\n",
            "INFO line 89:17: Changing keep_prob arg of tf.nn.dropout to rate\n",
            "\n",
            "INFO line 29:25: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 30:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 31:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 32:37: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 33:30: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 34:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 35:35: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 36:26: Added keywords to args of function 'tf.shape'\n",
            "WARNING line 37:33: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 37:33: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 37:130: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "INFO line 56:41: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 56:41: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 57:40: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 57:40: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 59:38: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 59:38: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 60:37: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 60:37: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 63:34: Added keywords to args of function 'tf.nn.embedding_lookup'\n",
            "WARNING line 63:34: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 75:50: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 76:47: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 80:50: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 81:47: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 100:43: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 107:25: Renamed 'tf.contrib.rnn.BasicLSTMCell' to 'tf.compat.v1.nn.rnn_cell.BasicLSTMCell'\n",
            "WARNING line 108:27: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n",
            "ERROR line 108:27: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "INFO line 111:27: Renamed 'tf.contrib.rnn.MultiRNNCell' to 'tf.compat.v1.nn.rnn_cell.MultiRNNCell'\n",
            "INFO line 171:21: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 185:21: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 196:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "WARNING line 197:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 197:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 197:79: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 198:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 198:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 198:78: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
            "\n",
            "WARNING line 199:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 199:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 199:62: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 199:62: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "WARNING line 200:17: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "INFO line 200:17: Renamed 'tf.get_variable' to 'tf.compat.v1.get_variable'\n",
            "INFO line 200:62: tf.zeros_initializer requires manual check. Initializers no longer have the dtype argument in the constructor or partition_info argument in the __call__ method.\n",
            "The calls have been converted to compat.v1 for safety (even though they may already have been correct).\n",
            "INFO line 200:62: Renamed 'tf.zeros_initializer' to 'tf.compat.v1.zeros_initializer'\n",
            "INFO line 218:22: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 227:16: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 230:20: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 238:25: Renamed 'tf.trainable_variables' to 'tf.compat.v1.trainable_variables'\n",
            "INFO line 239:47: Added keywords to args of function 'tf.gradients'\n",
            "INFO line 244:24: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 257:44: Renamed 'tf.batch_gather' to 'tf.compat.v1.batch_gather'\n",
            "INFO line 265:29: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 267:30: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 269:23: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 291:23: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 303:26: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 313:44: Renamed 'tf.batch_gather' to 'tf.compat.v1.batch_gather'\n",
            "INFO line 321:19: Added keywords to args of function 'tf.transpose'\n",
            "INFO line 322:19: Added keywords to args of function 'tf.transpose'\n",
            "INFO line 323:59: Added keywords to args of function 'tf.shape'\n",
            "INFO line 327:88: Added keywords to args of function 'tf.shape'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 5 files\n",
            "Detected 17 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "File: GIKT/model.py\n",
            "--------------------------------------------------------------------------------\n",
            "GIKT/model.py:37:33: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:56:41: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:57:40: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:59:38: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:60:37: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:63:34: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "GIKT/model.py:108:27: WARNING: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib.rnn. (Manual edit required) tf.contrib.rnn.* has been deprecated, and widely used cells/functions will be moved to tensorflow/addons repository. Please check it there and file Github issues if necessary.\n",
            "GIKT/model.py:108:27: ERROR: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "GIKT/model.py:197:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:198:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:199:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/model.py:200:17: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "--------------------------------------------------------------------------------\n",
            "File: GIKT/train.py\n",
            "--------------------------------------------------------------------------------\n",
            "GIKT/train.py:158:4: WARNING: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n",
            "--------------------------------------------------------------------------------\n",
            "File: GIKT/aggregators.py\n",
            "--------------------------------------------------------------------------------\n",
            "GIKT/aggregators.py:49:27: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/aggregators.py:51:24: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/aggregators.py:76:27: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "GIKT/aggregators.py:78:24: WARNING: tf.get_variable requires manual check. tf.get_variable returns ResourceVariables by default in 2.0, which have well-defined semantics and are stricter about shapes. You can disable this behavior by passing use_resource=False, or by calling tf.compat.v1.disable_resource_variables().\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'tree_report.txt'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXEH8C4DNa1"
      },
      "source": [
        "#*Test* Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaJNfaUjhWp",
        "outputId": "6547a61b-8c8e-41f0-869c-a719f00cee9a"
      },
      "source": [
        "!python /content/GIKT/main.py \\\n",
        "--dataset /content/data/assist09_3/assist09_3 \\\n",
        "--log_dir /content/logs \\\n",
        "--checkpoint_dir /content/checkpoint \\\n",
        "--n_hop 3 \\\n",
        "--skill_neighbor_num 4 \\\n",
        "--question_neighbor_num 4 \\\n",
        "--hist_neighbor_num 3 \\\n",
        "--next_neighbor_num 4 \\\n",
        "--model hsei \\\n",
        "--lr 0.001 \\\n",
        "--att_bound 0.7 \\\n",
        "--sim_emb question_emb \\\n",
        "--dropout_keep_probs [0.8,0.8,1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GIKT/main.py\", line 8, in <module>\n",
            "    from train import train\n",
            "  File \"/content/GIKT/train.py\", line 171\n",
            "    log_path =  # os.path.join(args.log_dir, name+model_dir+'.csv') #  os.path.join('/content/'args.log_dir +'.json')  #\n",
            "                                                                                                                       ^\n",
            "SyntaxError: invalid syntax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW6VLcpUq1mG"
      },
      "source": [
        "#Modules unit testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll5mqQEIW-Ao"
      },
      "source": [
        "logs         /train_/       content/data/assist09_3/assist09_3_hsei_0.001lr_3hop_4sn_4qn_3hn_4nn_question_emb_0.7bound_[0.8,0.8,1]keep_   1626538324.1748157.csv\n",
        "args.log_dir, \n",
        "/content/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "k-mGlws8VzMK",
        "outputId": "02a4e205-6078-4915-a021-88f84bf91d25"
      },
      "source": [
        "import os\n",
        "\n",
        "log_path = os.path.join('/content/'+ args.log_dir +'.json')\n",
        "print(log_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ad1d61b40b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMiCRUZPWbjS"
      },
      "source": [
        "# model.py\n",
        "\n",
        "# encoding:utf-8\n",
        "import tensorflow as tf\n",
        "#from aggregators import SumAggregator, ConcatAggregator\n",
        "\n",
        "\n",
        "class GIKT(object):\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.args = args\n",
        "        self.hidden_neurons = args.hidden_neurons\n",
        "        self.max_step = args.max_step - 1\n",
        "        self.feature_answer_size = args.feature_answer_size\n",
        "        self.field_size = args.field_size\n",
        "        self.embedding_size = args.embedding_size\n",
        "\n",
        "        self.dropout_keep_probs = eval(args.dropout_keep_probs)\n",
        "        self.select_index = args.select_index\n",
        "        self.hist_neighbor_num = args.hist_neighbor_num  # M\n",
        "        self.next_neighbor_num = args.next_neighbor_num  # N\n",
        "        self.lr = args.lr\n",
        "        self.n_hop = args.n_hop\n",
        "\n",
        "        self.question_neighbor_num = args.question_neighbor_num\n",
        "        self.skill_neighbor_num = args.skill_neighbor_num\n",
        "\n",
        "        self.question_neighbors = args.question_neighbors\n",
        "        self.skill_neighbors = args.skill_neighbors\n",
        "\n",
        "        self.log_dir = args.log_dir ## Added new\n",
        "\n",
        "        self.keep_prob = tf.placeholder(tf.float32)  # dropout keep prob\n",
        "        self.keep_prob_gnn = tf.placeholder(tf.float32)  # dropout keep prob\n",
        "        self.is_training = tf.placeholder(tf.bool)\n",
        "        self.features_answer_index = tf.placeholder(tf.int32, [None, self.max_step + 1, self.field_size])\n",
        "        self.target_answers = tf.placeholder(tf.float32, [None, self.max_step])\n",
        "        self.sequence_lens = tf.placeholder(tf.int32, [None])\n",
        "        self.hist_neighbor_index = tf.placeholder(tf.int32, [None, self.max_step, self.hist_neighbor_num])\n",
        "        self.batch_size = tf.shape(self.features_answer_index)[0]\n",
        "        self.feature_embedding = tf.get_variable(\"feature_embedding\", [self.feature_answer_size, self.embedding_size],initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        if args.aggregator == 'sum':\n",
        "            self.aggregator_class = SumAggregator\n",
        "        elif args.aggregator == 'concat':\n",
        "            self.aggregator_class = ConcatAggregator\n",
        "        else:\n",
        "            raise Exception(\"Unknown aggregator: \" + args.aggregator)\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        hidden_size = self.hidden_neurons[-1]\n",
        "        select_feature_index = tf.gather(self.features_answer_index, self.select_index, axis=-1)\n",
        "        select_size = len(self.select_index)\n",
        "        questions_index = select_feature_index[:, :-1, 1]\n",
        "        next_questions_index =select_feature_index[:,1:,1]\n",
        "        skill_index = select_feature_index[:,:-1,0]\n",
        "        next_skill_index =select_feature_index[:,1:,0]\n",
        "\n",
        "        self.input_questions_embedding = tf.nn.embedding_lookup(self.feature_embedding, questions_index) #[batch_size,seq_len,d]\n",
        "        self.next_questions_embedding = tf.nn.embedding_lookup(self.feature_embedding,next_questions_index) #[batch_size,seq_len,select_size-1,d]\n",
        "\n",
        "        self.input_skills_embedding = tf.nn.embedding_lookup(self.feature_embedding, skill_index) #[batch_size,seq_len,d]\n",
        "        self.next_skills_embedding = tf.nn.embedding_lookup(self.feature_embedding,next_skill_index) #[batch_size,seq_len,select_size-1,d]\n",
        "\n",
        "\n",
        "        input_answers_embedding = tf.nn.embedding_lookup(self.feature_embedding,select_feature_index[:,:-1,-1]) #[batch_size,seq_len,1,d]\n",
        "        input_answers_index = select_feature_index[:,:-1,-1]\n",
        "\n",
        "        if self.n_hop>0:\n",
        "            #gnn\n",
        "            input_neighbors = self.get_neighbors(self.n_hop,questions_index)##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "            aggregate_embedding,self.aggregators = self.aggregate(input_neighbors, self.input_questions_embedding)\n",
        "\n",
        "            next_input_neighbors = self.get_neighbors(self.n_hop,next_questions_index)##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "            next_aggregate_embedding,self.aggregators = self.aggregate(next_input_neighbors, self.next_questions_embedding)\n",
        "\n",
        "            feature_emb_size =  self.embedding_size\n",
        "            feature_trans_embedding  = tf.reshape(tf.layers.dense(tf.reshape(aggregate_embedding[0],[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu, name = 'feature_layer', reuse = False), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "            next_trans_embedding  = tf.reshape(tf.layers.dense(tf.reshape(next_aggregate_embedding[0],[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu, name = 'feature_layer', reuse = True), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "\n",
        "        else:\n",
        "            feature_emb_size =  self.embedding_size\n",
        "            feature_trans_embedding  = tf.reshape(tf.layers.dense(tf.reshape(self.input_questions_embedding,[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu, name = 'feature_layer', reuse = False), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "            next_trans_embedding  = tf.reshape(tf.layers.dense(tf.reshape(self.next_questions_embedding,[-1,feature_emb_size]),hidden_size, activation = tf.nn.relu, name = 'feature_layer', reuse = True), [-1,self.max_step, hidden_size]) #[batch_size,max_step,hidden_size]\n",
        "\n",
        "            # # gnn\n",
        "            input_neighbors = self.get_neighbors(1,\n",
        "                                                 questions_index)  ##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "\n",
        "            next_input_neighbors = self.get_neighbors(1,\n",
        "                                                      next_questions_index)  ##[[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]\n",
        "\n",
        "            next_aggregate_embedding = [next_trans_embedding,tf.reshape(tf.gather(self.feature_embedding, tf.reshape(next_input_neighbors[-1], [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])]\n",
        "            aggregate_embedding = [feature_trans_embedding,tf.reshape(tf.gather(self.feature_embedding, tf.reshape(input_neighbors[-1], [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        input_fa_embedding = tf.reshape(tf.concat([feature_trans_embedding,input_answers_embedding],-1),[-1,hidden_size+self.embedding_size]) #embedding_size*2\n",
        "        input_trans_embedding = tf.reshape(tf.layers.dense(input_fa_embedding, hidden_size),\n",
        "                                            [-1, self.max_step, hidden_size])\n",
        "\n",
        "\n",
        "        #create rnn cell\n",
        "        hidden_layers = []\n",
        "        for idx, hidden_size in enumerate(self.hidden_neurons):\n",
        "            lstm_layer = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size,name='input_rnn%d'%idx)\n",
        "            hidden_layer = tf.contrib.rnn.DropoutWrapper(cell=lstm_layer,\n",
        "                                                         output_keep_prob=self.keep_prob)\n",
        "            hidden_layers.append(hidden_layer)\n",
        "        self.hidden_cell = tf.contrib.rnn.MultiRNNCell(cells=hidden_layers, state_is_tuple=True)  # RNN\n",
        "\n",
        "        output_series = []\n",
        "        self.state = self.hidden_cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "        for i in range(self.max_step):\n",
        "            current_output, self.state = self.hidden_cell(input_trans_embedding[:, i, :], self.state)\n",
        "            output_series.append(current_output)\n",
        "\n",
        "        output_series = tf.reshape(tf.concat(output_series, 1), [-1, self.max_step, hidden_size])\n",
        "\n",
        "\n",
        "        if self.args.model == \"hssi\":\n",
        "            self.hist_neighbors_features = self.hist_neighbor_sampler(\n",
        "                output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "        elif self.args.model == \"hsei\":\n",
        "            self.hist_neighbors_features = self.hist_neighbor_sampler(\n",
        "                input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "        elif self.args.model == \"ssei\":\n",
        "            if self.args.sim_emb == \"skill_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_skills_embedding,\n",
        "                                                                           self.next_skills_embedding,\n",
        "                                                                           input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            elif self.args.sim_emb == \"question_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_questions_embedding,\n",
        "                                                                           self.next_questions_embedding,\n",
        "                                                                           input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            else:\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(feature_trans_embedding,\n",
        "                                                                           next_trans_embedding,\n",
        "                                                                           input_trans_embedding)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "        else:\n",
        "            if self.args.sim_emb == \"skill_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_skills_embedding,\n",
        "                                                                           self.next_skills_embedding,\n",
        "                                                                           output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            elif self.args.sim_emb == \"question_emb\":\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(self.input_questions_embedding,\n",
        "                                                                           self.next_questions_embedding,\n",
        "                                                                           output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "            else:\n",
        "                self.hist_neighbors_features = self.hist_neighbor_sampler1(feature_trans_embedding,\n",
        "                                                                           next_trans_embedding,\n",
        "                                                                           output_series)  # [self.batch_size,max_step,self.hist_neighbor_num,hidden_size]\n",
        "\n",
        "        if self.next_neighbor_num!=0:\n",
        "            Nn = self.next_neighbor_sampler(next_aggregate_embedding)  # [batch_size,max_step,N+1,embedding_size]\n",
        "            Nn = tf.concat([tf.expand_dims(next_trans_embedding,2),Nn],-2)\n",
        "            next_neighbor_num = self.next_neighbor_num+1\n",
        "        else:\n",
        "            Nn = tf.expand_dims(next_trans_embedding, 2)\n",
        "            next_neighbor_num = 1\n",
        "\n",
        "\n",
        "        if self.hist_neighbor_num != 0:\n",
        "\n",
        "\n",
        "            Nh = tf.concat([tf.expand_dims(output_series, 2), self.hist_neighbors_features],\n",
        "                           2)  # [self.batch_size,max_step,M+1,feature_trans_size]]\n",
        "\n",
        "            logits = tf.reduce_sum(tf.expand_dims(Nh, 3) * tf.expand_dims(Nn, 2),\n",
        "                                   axis=4)  # [-1,max_step,Nh,1,emb_size]*[-1,max_step,1,Nn,emb_size]\n",
        "\n",
        "\n",
        "\n",
        "            logits = tf.reshape(logits, [-1, self.max_step, (\n",
        "                        self.hist_neighbor_num + 1) * next_neighbor_num])  # ====>[batch_size,max_step,Nu*Nv]\n",
        "\n",
        "\n",
        "        else:\n",
        "\n",
        "            Nh = tf.expand_dims(output_series, 2)  # [self.batch_size,max_step,1,feature_trans_size]\n",
        "\n",
        "\n",
        "            logits = tf.reduce_sum(tf.expand_dims(Nh, 3) * tf.expand_dims(Nn, 2),\n",
        "                                   axis=4)  # [-1,max_step,Nh,1,emb_size]*[-1,max_step,1,Nn,emb_size]\n",
        "\n",
        "            logits = tf.reshape(logits,\n",
        "                                [-1, self.max_step, 1 * next_neighbor_num])  # ====>[batch_size,max_step,Nu*Nv]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope('ni'):\n",
        "            w1 = tf.get_variable('atn_weights_1',[hidden_size, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
        "            w2 = tf.get_variable('atn_weights_2',[hidden_size, 1],initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b1 = tf.get_variable('atn_bias_1',[1],initializer=tf.zeros_initializer())\n",
        "            b2 = tf.get_variable('atn_bias_2',[1],initializer=tf.zeros_initializer())\n",
        "        if select_size > 3:\n",
        "\n",
        "            f1 = tf.reshape(tf.matmul(tf.reshape(Nh, [-1, hidden_size]), w1) + b1,\n",
        "                            [-1, self.max_step, self.hist_neighbor_num + 1, 1])\n",
        "            f2 = tf.reshape(tf.matmul(tf.reshape(Nn, [-1, hidden_size]), w2) + b2,\n",
        "                            [-1, self.max_step, 1, next_neighbor_num])\n",
        "            coefs = tf.nn.softmax(tf.nn.tanh(\n",
        "                tf.reshape(f1 + f2, [-1, self.max_step, (self.hist_neighbor_num + 1) * next_neighbor_num])))  # temp=10\n",
        "        else:\n",
        "            f1 = tf.reshape(tf.matmul(tf.reshape(Nh, [-1, hidden_size]), w1) + b1,\n",
        "                            [-1, self.max_step, self.hist_neighbor_num + 1, 1])\n",
        "            f2 = tf.reshape(tf.matmul(tf.reshape(Nn, [-1, hidden_size]), w2) + b2,\n",
        "                            [-1, self.max_step, 1, next_neighbor_num])\n",
        "            coefs = tf.nn.softmax(tf.nn.tanh(\n",
        "                tf.reshape(f1 + f2, [-1, self.max_step, (self.hist_neighbor_num + 1) * next_neighbor_num])))  # temp=10\n",
        "\n",
        "        #coefs = tf.nn.softmax(logits)\n",
        "        self.logits = tf.reduce_sum(logits * coefs, axis=-1)\n",
        "\n",
        "        self.flat_target_logits = flat_target_logits = tf.reshape(self.logits, [-1])\n",
        "        self.flat_target_correctness = tf.reshape(self.target_answers, [-1])\n",
        "        self.pred = tf.sigmoid(tf.reshape(flat_target_logits, [-1, self.max_step]))\n",
        "        self.binary_pred = tf.cast(tf.greater_equal(self.pred, 0.5), tf.int32)\n",
        "\n",
        "        self.filling_seqs = tf.cast(tf.sequence_mask(self.sequence_lens - 1, self.max_step),\n",
        "                                    dtype=tf.float32)  # [batch_size,seq_len]\n",
        "        index = tf.where(tf.not_equal(tf.reshape(self.filling_seqs, [-1]), tf.constant(0, dtype=tf.float32)))\n",
        "        clear_flat_target_logits = tf.gather(self.flat_target_logits, index)\n",
        "        clear_flat_target_correctness = tf.gather(self.flat_target_correctness, index)\n",
        "        self.loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=clear_flat_target_correctness,\n",
        "                                                                          logits=clear_flat_target_logits))\n",
        "\n",
        "\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        # self.lr = tf.Variable(0.0, trainable=False)\n",
        "\n",
        "        trainable_vars = tf.trainable_variables()\n",
        "        self.grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars), 50)\n",
        "        # optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "        # optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "        # optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=0.95)\n",
        "        # self.train_op = optimizer.apply_gradients(zip(self.grads, trainable_vars))\n",
        "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
        "                                               beta1=0.9, beta2=0.999, epsilon=1e-8). \\\n",
        "            minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "        print(\"initialize complete\")\n",
        "\n",
        "\n",
        "\n",
        "    def hist_neighbor_sampler(self,input_embedding):\n",
        "        zero_embeddings = tf.expand_dims(tf.zeros([self.batch_size,self.hidden_neurons[-1]],dtype=tf.float32),1)#[batch_size,1,hidden_size]\n",
        "        input_embedding = tf.concat([input_embedding,zero_embeddings],1)#[batch_size,max_step+1,hidden_size]\n",
        "        #input_embedding:[batch_size,max_step,fa_trans_size]\n",
        "        temp_hist_index = tf.reshape(self.hist_neighbor_index,[-1,self.max_step*self.hist_neighbor_num]) #[self.batch_size, max_step*M]\n",
        "        hist_neighbors_features =tf.reshape(tf.batch_gather(input_embedding,temp_hist_index),[-1,self.max_step,self.hist_neighbor_num,input_embedding.shape[-1]])\n",
        "\n",
        "        #select last neigbor_num questions\n",
        "\n",
        "        return hist_neighbors_features\n",
        "\n",
        "    def hist_neighbor_sampler1(self, input_q_emb, next_q_emb, qa_emb):#sample based on question similarity\n",
        "        #next_q_emb:[batch_size,ms,emb_size]\n",
        "        mold_nextq = tf.sqrt(tf.reduce_sum(next_q_emb*next_q_emb,-1))#[bs,ms]\n",
        "        next_q_emb = tf.expand_dims(next_q_emb,2)\n",
        "        mold_inputq = tf.sqrt(tf.reduce_sum(input_q_emb*input_q_emb,-1))#[bs,ms]\n",
        "        input_q_emb = tf.expand_dims(input_q_emb,1)\n",
        "        q_similarity = tf.reduce_sum(next_q_emb*input_q_emb,-1)#[batch_size,ms,ms]\n",
        "        molds = tf.expand_dims(mold_nextq,2)*tf.expand_dims(mold_inputq,1)#[bs,ms,ms]\n",
        "        q_similarity = q_similarity/molds\n",
        "\n",
        "\n",
        "        zero_embeddings = tf.expand_dims(tf.zeros([self.batch_size,self.hidden_neurons[-1]],dtype=tf.float32),1)#[batch_size,1,hidden_size]\n",
        "        qa_emb = tf.concat([qa_emb, zero_embeddings], 1)#[batch_size,max_step+1,hidden_size]\n",
        "        paddings = tf.fill(value=-1,dims=[self.batch_size,self.hist_neighbor_num,self.hist_neighbor_num])\n",
        "\n",
        "\n",
        "        #mask future position\n",
        "        seq_mask = tf.range(1,self.max_step+1)\n",
        "        #input_qa_emb = tf.tile(tf.expand_dims(input_qa_emb,2),[1,1,self.max_step,1])\n",
        "        similarity_seqs = tf.tile(tf.expand_dims(tf.cast(tf.sequence_mask(seq_mask, self.max_step),\n",
        "                                    dtype=tf.float32),0),[self.batch_size,1,1])  # [batch_size,ms,ms]\n",
        "        #mask_seqs = tf.tile(tf.expand_dims(similarity_seqs,-1),[1,1,1,self.embedding_size])\n",
        "        #input_qa_emb = mask_seqs*input_qa_emb\n",
        "        q_similarity = q_similarity*similarity_seqs #only history q non zero# [batch_size,ms,ms]\n",
        "\n",
        "        #setting lower similarity bount\n",
        "        condition = tf.greater(q_similarity,self.args.att_bound)\n",
        "        #condition = tf.greater(q_similarity,0.9)\n",
        "        q_similarity = tf.where(condition,q_similarity,tf.zeros([self.batch_size,self.max_step,self.max_step]))\n",
        "        q_sim_index = tf.greater(q_similarity,0)#\n",
        "\n",
        "\n",
        "\n",
        "        self.q_similarity = q_similarity\n",
        "\n",
        "        temp_hist_index = tf.nn.top_k(q_similarity, self.hist_neighbor_num)[1]# [batch_size,ms,hist_num]\n",
        "        self.hist_attention_value = tf.nn.top_k(q_similarity, self.hist_neighbor_num)[0]# [batch_size,ms,hist_num]\n",
        "        #q_similarity[temp_hist_index]>0\n",
        "\n",
        "        #temp_hist_index = tf.where(self.hist_attention_value>self.args.att_bound,temp_hist_index,-1*tf.ones([self.batch_size,self.max_step,self.hist_neighbor_num],dtype=tf.int32))\n",
        "        temp_hist_index = tf.where(self.hist_attention_value>0,temp_hist_index,-1*tf.ones([self.batch_size,self.max_step,self.hist_neighbor_num],dtype=tf.int32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #temp_hist_index = tf.tile(tf.expand_dims(temp_hist_index[:,:,0],2),[1,1,self.hist_neighbor_num])\n",
        "        temp_hist_index = tf.reshape(temp_hist_index,[-1,self.max_step*self.hist_neighbor_num])\n",
        "        #self.temp_hist_index = tf.reshape(temp_hist_index, [-1, self.max_step, self.hist_neighbor_num])\n",
        "\n",
        "        hist_neighbors_features =tf.reshape(tf.batch_gather(qa_emb, temp_hist_index), [-1, self.max_step, self.hist_neighbor_num, qa_emb.shape[-1]])\n",
        "\n",
        "        return hist_neighbors_features\n",
        "\n",
        "\n",
        "    def next_neighbor_sampler(self,aggregate_embedding):\n",
        "\n",
        "        temp_emb = tf.reshape(aggregate_embedding[1],[-1,self.question_neighbor_num,self.embedding_size])\n",
        "        temp_emb = tf.transpose(temp_emb, [1, 0, 2])\n",
        "        temp_emb = tf.transpose(\n",
        "            tf.gather(temp_emb, tf.random.shuffle(tf.range(tf.shape(temp_emb)[0]))), [1, 0, 2])\n",
        "        if self.question_neighbor_num>=self.next_neighbor_num:\n",
        "            next_neighbors_embedding = tf.reshape(temp_emb[:,:self.next_neighbor_num,:],[self.batch_size,self.max_step,self.next_neighbor_num,self.embedding_size])\n",
        "        else:\n",
        "            tile_neighbor_embedding = tf.tile(temp_emb,[1, -(-self.next_neighbor_num // tf.shape(temp_emb)[0]), 1])\n",
        "            next_neighbors_embedding = tf.reshape(tile_neighbor_embedding[:,:self.next_neighbor_num,:],[self.batch_size,self.max_step,self.next_neighbor_num,self.embedding_size])\n",
        "\n",
        "        return next_neighbors_embedding\n",
        "\n",
        "    def get_neighbors(self,n_hop, question_index):\n",
        "        # question_index:[batch_size,seq_len]\n",
        "        # question_seed = tf.reshape(question_index#[batch_size*seq_len,1]\n",
        "        seeds = [question_index]  # [[batch_size,seq_len],[batch_size,seq_len,question_neighbor_num],batch_size,seq_len,question_neighbor_num,\n",
        "\n",
        "        for i in range(n_hop):\n",
        "            if i % 2 == 0:\n",
        "                neighbor = tf.reshape(tf.gather(self.question_neighbors, tf.reshape(seeds[i], [-1])),\n",
        "                                      [-1, self.max_step, self.question_neighbor_num])\n",
        "\n",
        "            else:\n",
        "                neighbor = tf.reshape(tf.gather(self.skill_neighbors, tf.reshape(seeds[i], [-1])),\n",
        "                                      [-1, self.max_step, self.skill_neighbor_num])\n",
        "\n",
        "\n",
        "            seeds.append(neighbor)  # [batch_size,seq_len,neighbor_num],[batch_size,seq_len,neighbor_num*neighbor_num]\n",
        "\n",
        "\n",
        "        return seeds\n",
        "\n",
        "    def aggregate(self, input_neighbors, input_questions_embedding):\n",
        "        # [[batch_size,seq_len],[batch_size,seq_len,q_neighbor_num],[batch_size,seq_len,q_neighbor_num*s_neighbor_num]]\n",
        "        sq_neighbor_vectors = []\n",
        "        for hop_i, neighbors in enumerate(input_neighbors):\n",
        "            if hop_i % 2 == 0:  # question\n",
        "                temp_neighbors = tf.reshape(tf.gather(self.feature_embedding, tf.reshape(neighbors, [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])\n",
        "                sq_neighbor_vectors.append(temp_neighbors)\n",
        "            else:  # skill\n",
        "                temp_neighbors = tf.reshape(tf.gather(self.feature_embedding, tf.reshape(neighbors, [-1])),\n",
        "                                            [self.batch_size, self.max_step, -1, self.embedding_size])\n",
        "                sq_neighbor_vectors.append(temp_neighbors)\n",
        "        aggregators = []\n",
        "        for i in range(self.n_hop):\n",
        "            if i == self.n_hop - 1:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.max_step, self.embedding_size, act=tf.nn.tanh,\n",
        "                                                   dropout=self.keep_prob_gnn)\n",
        "            else:\n",
        "                aggregator = self.aggregator_class(self.batch_size, self.max_step, self.embedding_size, act=tf.nn.tanh,\n",
        "                                                   dropout=self.keep_prob_gnn)\n",
        "            aggregators.append(aggregator)\n",
        "\n",
        "            # vectors_next_iter = []\n",
        "            for hop in range(self.n_hop - i):  # aggregate from outside to inside#layer\n",
        "                if hop % 2 == 0:\n",
        "                    shape = [self.batch_size, self.max_step, -1, self.question_neighbor_num, self.embedding_size]\n",
        "                    vector = aggregator(self_vectors=sq_neighbor_vectors[hop],\n",
        "                                        neighbor_vectors=tf.reshape(sq_neighbor_vectors[hop + 1], shape),\n",
        "                                        question_embeddings=sq_neighbor_vectors[hop],\n",
        "                                        )  # [batch_size,seq_len, -1, dim]\n",
        "                else:\n",
        "                    shape = [self.batch_size, self.max_step, -1, self.skill_neighbor_num, self.embedding_size]\n",
        "                    vector = aggregator(self_vectors=sq_neighbor_vectors[hop],\n",
        "                                        neighbor_vectors=tf.reshape(sq_neighbor_vectors[hop + 1], shape),\n",
        "                                        question_embeddings=sq_neighbor_vectors[hop],\n",
        "                                        )  # [batch_size,seq_len, -1, dim]\n",
        "                # shape = [self.batch_size, self.max_step, -1, self.sample_neighbor_num, self.embedding_size]\n",
        "\n",
        "                # vectors_next_iter.append(vector)\n",
        "                sq_neighbor_vectors[hop] = vector\n",
        "            # sq_neighbor_vectors = vectors_next_iter\n",
        "\n",
        "        # res = tf.reshape(sq_neighbor_vectors[0], [self.batch_size,self.max_step, self.embedding_size])\n",
        "        res = sq_neighbor_vectors  # [[batch_size,max_step,-1,embedding_size]...]\n",
        "\n",
        "        return res, aggregators\n",
        "\n",
        "    # step on batch\n",
        "    def train(self, sess, features_answer_index, target_answers, seq_lens, hist_neighbor_index):\n",
        "\n",
        "        input_feed = {self.features_answer_index: features_answer_index,\n",
        "                      self.target_answers: target_answers,\n",
        "                      self.sequence_lens: seq_lens,\n",
        "                      self.hist_neighbor_index: hist_neighbor_index,\n",
        "                      self.is_training: True}\n",
        "\n",
        "        input_feed[self.keep_prob] = self.dropout_keep_probs[0]\n",
        "        input_feed[self.keep_prob_gnn] = self.dropout_keep_probs[1]\n",
        "        # input_feed[self.aggregate_keep_prob] = self.dropout_keep_probs[1]\n",
        "\n",
        "        bin_pred, pred, train_loss, _, aaaa = sess.run(\n",
        "            [self.binary_pred, self.pred, self.loss, self.train_op, self.flat_target_correctness], input_feed)\n",
        "\n",
        "\n",
        "        return bin_pred, pred, train_loss\n",
        "\n",
        "    def evaluate(self, sess, features_answer_index, target_answers, seq_lens, hist_neighbor_index,evaluate_step):\n",
        "\n",
        "        input_feed = {self.features_answer_index: features_answer_index,\n",
        "                      self.target_answers: target_answers,\n",
        "                      self.sequence_lens: seq_lens,\n",
        "                      self.hist_neighbor_index: hist_neighbor_index,\n",
        "                      self.is_training: False}\n",
        "\n",
        "        input_feed[self.keep_prob] = self.dropout_keep_probs[-1]\n",
        "        input_feed[self.keep_prob_gnn] = self.dropout_keep_probs[-1]\n",
        "\n",
        "        bin_pred, pred = sess.run([self.binary_pred, self.pred], input_feed)\n",
        "\n",
        "        return bin_pred, pred\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx2AuCj8SHxH"
      },
      "source": [
        "#aggregators.py\n",
        "import tensorflow as tf\n",
        "from abc import abstractmethod\n",
        "\n",
        "LAYER_IDS = {}\n",
        "\n",
        "\n",
        "def get_layer_id(layer_name=''):\n",
        "    if layer_name not in LAYER_IDS:\n",
        "        LAYER_IDS[layer_name] = 0\n",
        "        return 0\n",
        "    else:\n",
        "        LAYER_IDS[layer_name] += 1\n",
        "        return LAYER_IDS[layer_name]\n",
        "\n",
        "\n",
        "class Aggregator(object):\n",
        "    def __init__(self, batch_size, seq_len,dim, dropout, act, name):\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_id(layer))\n",
        "        self.name = name\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.dim = dim\n",
        "\n",
        "    def __call__(self, self_vectors, neighbor_vectors, question_embeddings):\n",
        "        outputs = self._call(self_vectors, neighbor_vectors, question_embeddings)\n",
        "        return outputs\n",
        "\n",
        "    @abstractmethod\n",
        "    def _call(self, self_vectors, neighbor_vectors, question_embeddings):\n",
        "        # dimension:\n",
        "        # self_vectors: [batch_size, -1, dim]\n",
        "        # neighbor_vectors: [batch_size, -1, n_neighbor, dim]\n",
        "        # neighbor_relations: [batch_size, -1, n_neighbor, dim]\n",
        "        # user_embeddings: [batch_size, dim]\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SumAggregator(Aggregator):\n",
        "    def __init__(self, batch_size, seq_len,dim, dropout=0., act=tf.nn.relu, name=None):\n",
        "        super(SumAggregator, self).__init__(batch_size,seq_len,dim, dropout, act, name)\n",
        "\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.weights = tf.get_variable(\n",
        "                shape=[self.dim, self.dim], initializer=tf.contrib.layers.xavier_initializer(), name='weights')\n",
        "            self.bias = tf.get_variable(shape=[self.dim], initializer=tf.zeros_initializer(), name='bias')\n",
        "\n",
        "    def _call(self,self_vectors, neighbor_vectors, question_embeddings):\n",
        "        # [batch_size,seq_len, -1, dim]\n",
        "        neighbors_agg = tf.reduce_mean(neighbor_vectors, axis=-2)\n",
        "        output = tf.reshape(self_vectors + neighbors_agg, [-1, self.dim])\n",
        "        #neighbors_agg = tf.concat([tf.reshape(self_vectors,[self.batch_size,self.seq_len,-1,1,self.dim]),neighbor_vectors],-2)\n",
        "\n",
        "        # [-1, dim]\n",
        "        #output = tf.reshape(tf.reduce_mean(neighbors_agg,-2), [-1, self.dim])\n",
        "        output = tf.nn.dropout(output, keep_prob=self.dropout)\n",
        "        output = tf.matmul(output, self.weights) + self.bias\n",
        "\n",
        "        # [batch_size,seq_len, -1, dim]\n",
        "        output = tf.reshape(output, [self.batch_size,self.seq_len, -1, self.dim])\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n",
        "\n",
        "class ConcatAggregator(Aggregator):\n",
        "    def __init__(self, batch_size,seq_len, dim, dropout=0., act=tf.nn.relu, name=None):\n",
        "        super(ConcatAggregator, self).__init__(batch_size, seq_len,dim, dropout, act, name)\n",
        "\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.weights = tf.get_variable(\n",
        "                shape=[self.dim * 2, self.dim], initializer=tf.contrib.layers.xavier_initializer(), name='weights')\n",
        "            self.bias = tf.get_variable(shape=[self.dim], initializer=tf.zeros_initializer(), name='bias')\n",
        "\n",
        "    def _call(self, self_vectors, neighbor_vectors, question_embeddings):\n",
        "        # [batch_size,seq_len, -1, dim]\n",
        "        neighbors_agg = tf.reduce_mean(neighbor_vectors, axis=-2)\n",
        "\n",
        "        # [batch_size,seq_len, -1, dim * 2]\n",
        "        output = tf.concat([self_vectors, neighbors_agg], axis=-1)\n",
        "\n",
        "        # [-1, dim * 2]\n",
        "        output = tf.reshape(output, [-1, self.dim * 2])\n",
        "        output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
        "\n",
        "        # [-1, dim]\n",
        "        output = tf.matmul(output, self.weights) + self.bias\n",
        "\n",
        "        # [batch_size, -1, dim]\n",
        "        output = tf.reshape(output, [self.batch_size,self.seq_len, -1, self.dim])\n",
        "\n",
        "        return self.act(output)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2pFRC-NRC64"
      },
      "source": [
        "# train.py\n",
        "from model import GIKT\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
        "from data_process import DataGenerator\n",
        "\n",
        "def train(args,train_dkt):\n",
        "    run_config = tf.ConfigProto()\n",
        "    run_config.gpu_options.allow_growth = True\n",
        "\n",
        "    with tf.Session(config=run_config) as sess:\n",
        "\n",
        "        print(args.model)\n",
        "\n",
        "        model = GIKT(args)\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        index = 0\n",
        "\n",
        "        if train_dkt:\n",
        "            # lr = 0.4\n",
        "            # lr_decay = 0.92\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            model_dir = save_model_dir(args)\n",
        "\n",
        "            best_valid_auc = 0\n",
        "\n",
        "            for epoch in tqdm(range(args.num_epochs)):\n",
        "                train_generator = DataGenerator(args.train_seqs, args.max_step, batch_size=args.batch_size,\n",
        "                                                feature_size=args.feature_answer_size - 2,\n",
        "                                                hist_num=args.hist_neighbor_num)\n",
        "                valid_generator = DataGenerator(args.valid_seqs, args.max_step, batch_size=args.batch_size,\n",
        "                                                feature_size=args.feature_answer_size - 2,\n",
        "                                                hist_num=args.hist_neighbor_num)\n",
        "                #    assign_lr()\n",
        "                print(\"epoch:\", epoch)\n",
        "                # self.assign_lr(self.sess,self.args.lr * self.args.lr_decay ** epoch)\n",
        "                overall_loss = 0\n",
        "                train_generator.shuffle()\n",
        "                preds, binary_preds, targets = list(), list(), list()\n",
        "                train_step = 0\n",
        "                while not train_generator.end:\n",
        "                    train_step += 1\n",
        "\n",
        "                    [features_answer_index,target_answers,seq_lens,hist_neighbor_index] = train_generator.next_batch()\n",
        "                    binary_pred, pred, loss = model.train(sess,features_answer_index,target_answers,seq_lens,hist_neighbor_index)\n",
        "\n",
        "                    overall_loss += loss\n",
        "                    for seq_idx, seq_len in enumerate(seq_lens):\n",
        "                        preds.append(pred[seq_idx, 0:seq_len])\n",
        "                        binary_preds.append(binary_pred[seq_idx, 0:seq_len])\n",
        "                        targets.append(target_answers[seq_idx, 0:seq_len])\n",
        "                # print(\"\\r idx:{0}, overall_loss:{1}\".format(train_generator.pos, overall_loss)),\n",
        "                train_loss = overall_loss / train_step\n",
        "                preds = np.concatenate(preds)\n",
        "                binary_preds = np.concatenate(binary_preds)\n",
        "                targets = np.concatenate(targets)\n",
        "                auc_value = roc_auc_score(targets, preds)\n",
        "                accuracy = accuracy_score(targets, binary_preds)\n",
        "                precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
        "                print(\"\\ntrain loss = {0},auc={1}, accuracy={2}\".format(train_loss, auc_value, accuracy))\n",
        "                write_log(args,model_dir,auc_value, accuracy, epoch, name='train_')\n",
        "\n",
        "                # if epoch == self.args.num_epochs-1:\n",
        "                #     self.save(epoch)\n",
        "\n",
        "                # valid\n",
        "                valid_generator.reset()\n",
        "                preds, binary_preds, targets = list(), list(), list()\n",
        "                valid_step = 0\n",
        "                #overall_loss = 0\n",
        "                while not valid_generator.end:\n",
        "                    valid_step += 1\n",
        "                    [features_answer_index,target_answers,seq_lens,hist_neighbor_index] = valid_generator.next_batch()\n",
        "                    binary_pred, pred = model.evaluate(sess,features_answer_index,target_answers,seq_lens,hist_neighbor_index,valid_step)\n",
        "                    #overall_loss += loss\n",
        "                    for seq_idx, seq_len in enumerate(seq_lens):\n",
        "                        preds.append(pred[seq_idx, 0:seq_len])\n",
        "                        binary_preds.append(binary_pred[seq_idx, 0:seq_len])\n",
        "                        targets.append(target_answers[seq_idx, 0:seq_len])\n",
        "                # compute metrics\n",
        "                #valid_loss = overall_loss / valid_step\n",
        "                preds = np.concatenate(preds)\n",
        "                binary_preds = np.concatenate(binary_preds)\n",
        "                targets = np.concatenate(targets)\n",
        "                auc_value = roc_auc_score(targets, preds)\n",
        "                accuracy = accuracy_score(targets, binary_preds)\n",
        "                precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
        "                print(\"\\nvalid auc={0}, accuracy={1}, precision={2}, recall={3}\".format(auc_value, accuracy, precision,\n",
        "                                                                                        recall))\n",
        "\n",
        "\n",
        "                write_log(args,model_dir,auc_value, accuracy, epoch, name='valid_')\n",
        "\n",
        "                if auc_value > best_valid_auc:\n",
        "                    print('%3.4f to %3.4f' % (best_valid_auc, auc_value))\n",
        "                    best_valid_auc = auc_value\n",
        "                    best_epoch = epoch\n",
        "                    #np.save('feature_embedding.npy', feature_embedding)\n",
        "                    checkpoint_dir = os.path.join(args.checkpoint_dir, model_dir)\n",
        "                    save(best_epoch,sess,checkpoint_dir,saver)\n",
        "                print(\"model_dir: \" + model_dir) # comment#\n",
        "                print(model_dir+\"\\t\"+str(best_valid_auc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "            if self.load():\n",
        "                print('CKPT loaded')\n",
        "            else:\n",
        "                raise Exception('CKPT need')\n",
        "            test_data_generator = DataGenerator(args.test_seqs, args.max_step, batch_size=args.batch_size,\n",
        "                                                feature_size=args.feature_answer_size - 2,\n",
        "                                                hist_num=args.hist_neighbor_num)\n",
        "            data_generator.reset()\n",
        "\n",
        "            correct_times = np.zeros(self.num_skills + 1)\n",
        "            preds, binary_preds, targets = list(), list(), list()\n",
        "            while not test_data_generator.end:\n",
        "\n",
        "                [features_answer_index, target_answers, seq_lens, hist_neighbor_index] = valid_generator.next_batch()\n",
        "                binary_pred, pred = model.evaluate(sess, features_answer_index, target_answers, seq_lens,\n",
        "                                                   hist_neighbor_index)\n",
        "                # overall_loss += loss\n",
        "                for seq_idx, seq_len in enumerate(seq_lens):\n",
        "                    preds.append(pred[seq_idx, 0:seq_len])\n",
        "                    binary_preds.append(binary_pred[seq_idx, 0:seq_len])\n",
        "                    targets.append(target_answers[seq_idx, 0:seq_len])\n",
        "\n",
        "\n",
        "\n",
        "            preds = np.concatenate(preds)\n",
        "            binary_preds = np.concatenate(binary_preds)\n",
        "            targets = np.concatenate(targets)\n",
        "            auc_value = roc_auc_score(targets, preds)\n",
        "            accuracy = accuracy_score(targets, binary_preds)\n",
        "            precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
        "            print(\"\\ntest auc={0}, accuracy={1}, precision={2}, recall={3}\".format(auc_value, accuracy, precision,\n",
        "                                                                                   recall))\n",
        "            print(model_dir)\n",
        "            write_log(args, model_dir, auc_value, accuracy, epoch, name='test_')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save(global_step,sess,checkpoint_dir,saver):\n",
        "    model_name = 'GIKT'\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.mkdir(checkpoint_dir)\n",
        "    saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=global_step)\n",
        "    print('Save checkpoint at %d' % (global_step))\n",
        "\n",
        "\n",
        "def save_model_dir(args):\n",
        "    return '{}_{}_{}lr_{}hop_{}sn_{}qn_{}hn_{}nn_{}_{}bound_{}keep_{}'.format(args.dataset,\n",
        "                                                args.model,args.lr,args.n_hop,args.skill_neighbor_num,args.question_neighbor_num,args.hist_neighbor_num,\\\n",
        "                                                                     args.next_neighbor_num,args.sim_emb,args.att_bound,args.dropout_keep_probs,args.tag)\n",
        "\n",
        "\n",
        "\n",
        "def write_log(args,model_dir,auc, accuracy, epoch, name='train_'):\n",
        "\n",
        "    log_path =  os.path.join(args.log_dir, name+model_dir+'.csv') #  os.path.join('/content/'args.log_dir +'.json')  #\n",
        "    if not os.path.exists(log_path):\n",
        "        \n",
        "        log_file = open(log_path, 'w')\n",
        "        log_file.write('Epoch\\tAuc\\tAccuracy\\n')\n",
        "    else:\n",
        "        log_file = open(log_path, 'a')\n",
        "\n",
        "    log_file.write(str(epoch) + '\\t' + str(auc) + '\\t' + str(accuracy)  + '\\n')\n",
        "    log_file.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}